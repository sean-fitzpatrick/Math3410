<!DOCTYPE html>
<html lang="en-US">
<!--********************************************-->
<!--*       Generated from PreTeXt source      *-->
<!--*                                          *-->
<!--*         https://pretextbook.org          *-->
<!--*                                          *-->
<!--********************************************-->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="robots" content="noindex, nofollow">
</head>
<body class="ignore-math">
<article class="hiddenproof"><h4 class="heading"><span class="title">Strategy.</span></h4> <div class="para">Any proof of linear independence should start by defining our set of vectors, and assuming that a linear combination of these vectors is equal to the zero vector, with the goal of showing that the scalars have to be zero.</div> <div class="para">Set up the equation (say, <span class="process-math">\(c_1\vv_1+\cdots c_n\vv_n=\zer\)</span>), with the assumption that your set of vectors is orthogonal. What happens if you take the dot product of both sides with one of these vectors?</div></article><span class="incontext"><a href="sec-orthogonal-sets.html#proof-40" class="internal">in-context</a></span>
</body>
</html>
