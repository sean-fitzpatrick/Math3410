<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="sec-dimension">
  <title>Basis and dimension</title>

  <p>
    Next, we begin with an important result, sometimes known as the <q>Fundamental Theorem</q>:
  </p>

  <theorem xml:id="theorem-steinitz">
    <title>Fundamental Theorem (Steinitz Exchange Lemma)</title>
    <statement>
      <p>
        Suppose <m>V = \spn\{\vv_1,\ldots, \vv_n\}</m>.
        If <m>\{\ww_1,\ldots, \ww_m\}</m> is a linearly independent set of vectors in <m>V</m>,
        then <m>m\leq n</m>.
      </p>
    </statement>
  </theorem>

  <p>
    If a set of vectors spans a vector space <m>V</m>, and it is not independent,
    we observed that it is possible to remove a vector from the set and still span <m>V</m>.
    This suggests that spanning sets that are also linearly independent are of particular importance,
    and indeed, they are important enough to have a name.
  </p>

  <definition xml:id="def-basis">
    <statement>
      <p>
        Let <m>V</m> be a vector space. A set <m>\mathcal{B}=\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m>
        is called a <term>basis</term> of <m>V</m> if <m>\mathcal{B}</m> is linearly independent,
        and <m>\operatorname{span}\mathcal{B} = V</m>.
      </p>
    </statement>
  </definition>

  <p>
    The importance of a basis is that vector vector <m>\vv\in V</m> can be written in terms of the basis,
    and this expression as a linear combination of basis vectors is <em>unique</em>.
    Another important fact is that every basis has the same number of elements.
  </p>

  <theorem xml:id="thm-invariance">
    <title>Invariance Theorem</title>

    <statement>
      <p>
        If <m>\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m> and <m>\{\mathbf{f}_1,\ldots, \mathbf{f}_m\}</m>
        are both bases of a vector space <m>V</m>, then <m>m=n</m>.
      </p>
    </statement>
  </theorem>

  <p>
    Suppose <m>V=\spn\{\vv_1,\ldots,\vv_n\}</m>.
    If this set is not linearly independent, <xref ref="theorem-surplus-span">Theorem</xref>
    tells us that we can remove a vector from the set, and still span <m>V</m>.
    We can repeat this procedure until we have a linearly independent set of vectors, which will then be a basis.
    These results let us make a definition.
  </p>

  <definition xml:id="def-dimension">
    <statement>
      <p>
        Let <m>V</m> be a vector space.
        If <m>V</m> can be spanned by a finite number of vectors,
        then we call <m>V</m> a <term>finite-dimensional</term> vector space.
        If <m>V</m> is finite-dimensional (and non-trivial), and <m>\{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m>
        is a basis of <m>V</m>, we say that <m>V</m> has <term>dimension</term> <m>n</m>,
        and write
        <me>
          \dim V = n
        </me>.
        If <m>V</m> cannot be spanned by finitely many vectors, we say that <m>V</m>
        is <term>infinite-dimensional</term>.
      </p>
    </statement>
  </definition>

  <exercise>
    <statement>
      <p>
        Find a basis for <m>U=\{X\in M_{22} \,|\, XA = AX\}</m>, if <m>A = \bbm 1\amp 1\\0\amp 0\ebm</m>
      </p>
    </statement>
    <solution>
      <p>
        Let <m>X=\bbm a\amp b\\c\amp d\ebm</m>. Then <m>AX = \bbm a+c\amp b+d\\0\amp 0\ebm</m>,
        and <m>XA = \bbm a\amp a\\c\amp c\ebm</m>, so the condition <m>AX=XA</m>
        requires:
        <md>
          <mrow>a+c \amp = a</mrow>
          <mrow>b+d \amp = a</mrow>
          <mrow>0 \amp = c</mrow>
          <mrow>0 \amp =c</mrow>
        </md>.
      </p>

      <p>
        So <m>c=0</m>, in which case the first equation <m>a=a</m> is trivial,
        and we are left with the single equation <m>a=b+d</m>. Thus, our matrix <m>X</m>
        must be of the form
        <me>
          X = \bbm b+d \amp b\\0\amp d\ebm = b\bbm 1\amp 1\\0\amp 0\ebm + d\bbm 1\amp 0\\0\amp 1\ebm
        </me>.
        Since the matrices <m>\bbm 1\amp 1\\0\amp 0\ebm</m> and <m>\bbm 1\amp 0\\0\amp 1\ebm</m>
        are not scalar multiples, they must be independent, and therefore, they form a basis for <m>U</m>.
      </p>
    </solution>
  </exercise>

  <example>
    <title>Standard bases</title>
    <statement>
      <p>
        Most of the vector spaces we work with come equipped with a <em>standard basis</em>.
        The standard basis for a vector space is typically a basis such that the scalars needed to express a vector in terms of that basis are the same scalars used to define the vector in the first place.
        For example, we write an element of <m>\R^3</m> as <m>(x,y,z)</m> (or <m>\langle x,y,z\rangle</m>,
        or <m>\begin{bmatrix}x\amp y\amp z\end{bmatrix}</m>, or <m>\begin{bmatrix}x\\y\\z\end{bmatrix}</m><ellipsis/>).
        We can also write
        <me>
          (x,y,z)=x(1,0,0)+y(0,1,0)+z(0,0,1)
        </me>.
        The set <m>\{(1,0,0),(0,1,0),(0,0,1)\}</m> is the standard basis for <m>\R^3</m>.
        In general, the vector space <m>\R^n</m> (written this time as column vectors) has standard basis
        <me>
          \vece_1=\bbm 1\\0\\ \vdots \\0\ebm, \vece_2 = \bbm 0\\1\\ \vdots \\0\ebm, \ldots, \vece_n=\bbm 0\\0\\ \vdots \\1\ebm
        </me>.
        From this, we can conclude (unsurprisingly) that <m>\dim \R^n = n</m>.
      </p>

      <p>
        Similarly, a polynomial <m>p(x)\in P_n(\R)</m> is usually written as
        <me>
          p(x) = a_0+a_1x+a_2x^2+\cdots + a_nx^n
        </me>,
        suggesting the standard basis <m>\{1,x,x^2,\ldots, x^n\}</m>.
        As a result, we see that <m>\dim P_n(\R)=n+1</m>.
      </p>

      <p>
        For one more example, we note that a <m>2\times 2</m> matrix <m>A\in M_{22}(\R)</m>
        can be written as
        <me>
          \bbm a\amp b\\c\amp d\ebm = a\bbm 1\amp 0\\0\amp 0\ebm+b\bbm 0\amp 1\\0\amp 0\ebm +c\bbm 0\amp 0\\1\amp 0\ebm + d\bbm 0\amp 0\\0\amp 1\ebm
        </me>,
        which suggests a standard basis for <m>M_{22}(\R)</m>, with similar results for any other matrix space.
        From this, we can conclude (exercise) that <m>\dim M_{mn}(\R)=mn</m>.
      </p>
    </statement>
    <solution>
      <p>

      </p>
    </solution>
  </example>

  <exercise>
    <introduction>
      <p>
        Show that the following sets are bases of <m>\R^3</m>.
      </p>
    </introduction>
    <task>
      <statement>
        <p>
          <m>\{(1,1,0),(1,0,1),(0,1,1)\}</m>
        </p>
      </statement>
      <solution>
        <p>
          We need to show that the set is independent, and that it spans.
        </p>
        <p>
          The set is independent if the equation
          <me>
            x(1,1,0)+y(1,0,1)+z(0,1,1)=(0,0,0)
          </me>
          has <m>x=y=z=0</m> as its only solution.
          This equation is equivalent to the system
          <md>
            <mrow>x+y \amp =0</mrow>
            <mrow>x+z \amp =0</mrow>
            <mrow>y+z \amp =0</mrow>
          </md>.
        </p>
        <p>
          We know that the solution to this system is unique if the coefficient matrix
          <m>A = \bbm 1\amp 1\amp 0\\1\amp 0\amp 1\\0\amp 1\amp 1\ebm</m> is invertible.
          Note that the columns of this matrix are vectors in our set.
        </p>
        <p>
          We can determine invertibility either by showing that the <init>RREF</init> of <m>A</m>
          is the identity, or by showing that the determinant of <m>A</m> is nonzero.
          Either way, this is most easily done by the computer:
        </p>

        <sage>
          <input>
            from sympy import Matrix,init_printing
            init_printing()
            A = Matrix([[1,1,0],[1,0,1],[0,1,1]])
            A.rref(), A.det()
          </input>
          <output>
            \[\left(\bbm 1\amp 0\amp 0\\0\amp 1\amp 0\\0\amp 0\amp 1\ebm, (0,1,2)\right), -2\]
          </output>
        </sage>

        <p>
          Our set of vectors is therefore linearly independent.
          Now, to show that it spans, we need to show that for any vector <m>(a,b,c)</m>,
          the equation
          <me>
            x(1,1,0)+y(1,0,1)+z(0,1,1)=(a,b,c)
          </me>
          has a solution. But we know that this system has the same coefficient matrix as the one above,
          and that existence of a solution again follows from invertibility of <m>A</m>,
          which we have already established.
        </p>
        <p>
          Note that for three vectors in <m>\R^3</m>, once independence has been confirmed,
          span is automatic. We will soon see that this is not a coincidence.
        </p>
      </solution>
    </task>
    <task>
      <statement>
        <p>
          <m>\{(-1,1,1),(1,-1,1),(1,1,-1)\}</m>
        </p>
      </statement>
      <solution>
        <p>
          Based on what we learned from the first set,
          determining whether or not this set is a basis is equivalent to determining whether or not the matrix
          <m>A</m> whose columns consist of the vectors in the set is invertible.
          We form the matrix
          <me>
            A = \bbm -1\amp 1\amp 1\\1\amp -1\amp 1\\1\amp 1\amp -1\ebm
          </me>
          and then check invertibility using the computer.
        </p>

        <sage>
          <input>
            A = Matrix([[-1,1,1],[1,-1,1],[1,1,-1]])
            A.det()
          </input>
          <output>
            \[4\]
          </output>
        </sage>

        <p>
          Since the determinant is nonzero, our set is a basis.
        </p>
      </solution>
    </task>
  </exercise>

  <p>
    The next two exercises are left to the reader to solve.
    In each case, your goal should be to turn the questions of independence and span into a system of equations,
    which you can then solve using the computer.
  </p>

  <exercise>
    <statement>
      <p>
        Show that the following is a basis of <m>M_{22}</m>:
        <me>
          \left\{\bbm 1\amp 0\\0\amp 1\ebm, \bbm 0\amp 1\\1\amp 0\ebm, \bbm 1\amp 1\\0\amp 1\ebm, \bbm 1\amp 0\\0\amp 0\ebm\right\}
        </me>.
      </p>
    </statement>
  </exercise>

  <sage>

  </sage>

  <exercise>
    <statement>
      <p>
        Show that <m>\{1+x,x+x^2,x^2+x^3,x^3\}</m> is a basis for <m>P_3</m>.
      </p>
    </statement>
  </exercise>

  <sage>

  </sage>

  <exercise>
    <statement>
      <p>
        Find a basis and dimension for the following subspaces of <m>P_2</m>:
        <ol label='a'>
          <li>
            <m>U_1 = \{a(1+x)+b(x+x^2)\,|\, a,b\in\R\}</m>
          </li>
          <li>
            <m>U_2=\{p(x)\in P_2 \,|\, p(1)=0\}</m>
          </li>
          <li>
            <m>U_3 = \{p(x)\in P_2 \,|\, p(x)=p(-x)\}</m>
          </li>
        </ol>
      </p>
    </statement>
    <solution>
      <p>
        <ol label='a'>
          <li>
            <p>
              By definition, <m>U_1 = \spn \{1+x,x+x^2\}</m>,
              and these vectors are independent, since neither is a scalar multiple of the other.
              Since there are two vectors in this basis, <m>\dim U_1 = 2</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>p(1)=0</m>, then <m>p(x)=(x-1)q(x)</m> for some polynomial <m>q</m>.
              Since <m>U_2</m> is a subspace of <m>P_2</m>, the degree of <m>q</m> is at most 2.
              Therefore, <m>q(x)=ax+b</m> for some <m>a,b\in\R</m>, and
              <me>
                p(x) = (x-1)(ax+b) = a(x^2-x)+b(x-1)
              </me>.
              Since <m>p</m> was arbitrary, this shows that <m>U_2 = \spn\{x^2-x,x-1\}</m>.
            </p>

            <p>
              The set <m>\{x^2-x,x-1\}</m> is also independent,
              since neither vector is a scalar multiple of the other.
              Therefore, this set is a basis, and <m>\dim U_2=2</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>p(x)=p(-x)</m>, then <m>p(x)</m> is an even polynomial,
              and therefore <m>p(x)=a+bx^2</m> for <m>a,b\in\R</m>.
              (If you didn't know this it's easily verified: if
              <me>
                a+bx+cx^2 = a+b(-x)+c(-x)^2
              </me>,
              we can immediately cancel <m>a</m> from each side,
              and since <m>(-x)^2=x^2</m>, we can cancel <m>cx^2</m> as well.
              This leaves <m>bx=-bx</m>, or <m>2bx=0</m>, which implies that <m>b=0</m>.)
            </p>

            <p>
              It follows that the set <m>\{1,x^2\}</m> spans <m>U_3</m>,
              and since this is a subset of the standard basis <m>\{1,x,x^2\}</m> of <m>P_2</m>,
              it must be independent, and is therefore a basis of <m>U_3</m>,
              letting us conclude that <m>\dim U_3=2</m>.
            </p>
          </li>
        </ol>
      </p>
    </solution>
  </exercise>

  <p>
    We've noted a few times now that if <m>\ww\in\spn\{\vv_1,\ldots, \vv_n\}</m>,
    then
    <me>
      \spn\{\ww,\vv_1,\ldots, \vv_n\}=\spn\{\vv_1,\ldots, \vv_n\}
    </me>
    If <m>\ww</m> is not in the span, we can make another useful observation:
  </p>

  <lemma xml:id="lemma-independent">
    <title>Independent Lemma</title>
    <statement>
      <p>
        Suppose <m>\{\vv_1,\ldots, \vv_n\}</m> is a linearly independent set of vectors in a vector space <m>V</m>.
        If <m>\uu\in V</m> but <m>\uu\notin \spn\{\vv_1,\ldots, \vv_n\}</m>,
        then <m>\{\uu,\vv_1,\ldots, \vv_n\}</m> is independent.
      </p>
    </statement>
    <proof>
      <p>
        Suppose <m>S=\{\vv_1,\ldots, \vv_n\}</m> is independent,
        and that <m>\uu\notin\spn S</m>. Suppose we have
        <me>
          a\uu+c_1\vv_1+c_2\vv_2+\cdots +c_n\mathbf{b}_n=\mathbf{0}
        </me>
        for scalars <m>a,c_1,\ldots, c_n</m>. We must have <m>a=0</m>;
        otherwise, we can multiply by <m>\frac1a</m> and rearrange to obtain
        <me>
          \uu = -\frac{c_1}{a}\vv_1-\cdots -\frac{c_n}{a}\vv_n
        </me>,
        but this would mean that <m>\uu\in \spn S</m>, contradicting our assumption.
      </p>

      <p>
        With <m>a=0</m> we're left with
        <me>
          c_1\vv_1+c_2\vv_2+\cdots +c_n\mathbf{b}_n=\mathbf{0}
        </me>,
        and since we assumed that the set <m>S</m> is independent,
        we must have <m>c_1=c_2=\cdots=c_n=0</m>. Since we already showed <m>a=0</m>,
        this shows that <m>\{\uu,\vv_1,\ldots, \vv_n\}</m> is independent.
      </p>
    </proof>

  </lemma>
  <p>
    This is, in fact, an <q>if and only if</q> result.
    If <m>\uu\in\spn\{\vv_1,\ldots, \vv_n\}</m>, then <m>\{\uu,\vv_1,\ldots, \vv_n\}</m> is not independent.
    Above, we argued that if <m>V</m> is finite dimensional,
    then any spanning set for <m>V</m> can be reduced to a basis.
    It probably won't surprise you that the following is also true.
  </p>

  <lemma xml:id="lem-enlarge-independent">
    <statement>
      <p>
        Let <m>V</m> be a finite-dimensional vector space,
        and let <m>U</m> be any subspace of <m>V</m>.
        Then any independent set of vectors <m>\{\uu_1,\ldots, \uu_k\}</m> in <m>U</m>
        can be enlarged to a basis of <m>U</m>.
      </p>
    </statement>
    <proof>
      <p>
        This follows from <xref ref="lemma-independent"/>.
        If our independent set of vectors spans <m>U</m>, then it's a basis and we're done.
        If not, we can find some vector not in the span,
        and add it to our set to obtain a larger set that is still independent.
        We can continue adding vectors in this fashion until we obtain a spanning set.
      </p>

      <p>
        Note that this process <em>must</em> terminate: <m>V</m> is finite-dimensional,
        so there is a finite spanning set for <m>V</m>, and therefore for <m>U</m>.
        By the Steinitz Exchange lemma, our independent set cannot get larger than this spanning set.
      </p>
    </proof>

  </lemma>

  <theorem xml:id="thm-basis-exist">
    <statement>
      <p>
        Any finite-dimensional vector space <m>V</m> has a basis.
        Moreover:
        <ol>
          <li>
            <p>
              If <m>V</m> can be spanned by <m>m</m> vectors,
              then <m>\dim V\leq m</m>.
            </p>
          </li>
          <li>
            <p>
              Given an independent set <m>I</m> in <m>V</m>,
              and a basis <m>\mathcal{B}</m> of <m>V</m>,
              we can enlarge <m>I</m> to a basis of <m>V</m> by adding elements of <m>\mathcal{B}</m>.
            </p>
          </li>
        </ol>
      </p>

      <p>
        If <m>U</m> is a subspace of <m>V</m>, then:
        <ol>
          <li>
            <p>
              <m>U</m> is finite-dimensional, and <m>\dim U\leq \dim V</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>\dim U = \dim V</m>, then <m>U=V</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
  </theorem>

  <exercise>
    <statement>
      <p>
        Find a basis of <m>M_{22}(\R)</m> that contains the vectors
        <me>
          \vv=\bbm 1\amp 1\\0\amp 0\ebm, \ww=\bbm 0\amp 1\\0\amp 1\ebm
        </me>.
      </p>
    </statement>
    <solution>
      <p>
        By the previous theorem, we can form a basis by adding vectors from the standard basis
        <me>
          \left\{\bbm 1\amp 0\\0\amp 0\ebm, \bbm 0\amp 1\\0\amp 0\ebm, \bbm 0\amp 0\\1\amp 0\ebm, \bbm 0\amp 0\\0\amp 1\ebm\right\}
        </me>.
        It's easy to check that <m>\bbm 1\amp 0\\0\amp 0\ebm</m> is not in the span of <m>\{\vv,\ww\}</m>.
        To get a basis, we need one more vector.
        Observe that all three of our vectors so far have a zero in the <m>(2,1)</m>-entry.
        Thus, <m>\bbm 0\amp 0\\1\amp 0\ebm</m> cannot be in the span of the first three vectors,
        and adding it gives us our basis.
      </p>
    </solution>
  </exercise>

  <exercise>
    <statement>
      <p>
        Extend the set <m>\{1+x,x+x^2,x-x^3\}</m> to a basis of <m>P_3(\R)</m>.
      </p>
    </statement>
    <solution>
      <p>
        Again, we only need to add one vector from the standard basis
        <m>\{1,x,x^2,x^3\}</m>, and it's not too hard to check that any of them will do.
      </p>
    </solution>
  </exercise>

  <exercise>
    <statement>
      <p>
        Give two examples of infinite-dimensional vector spaces.
        Support your answer.
      </p>
    </statement>
  </exercise>

  <p>
    Let's recap our results so far:
    <ul>
      <li>
        <p>
          A basis for a vector space <m>V</m> is an independent set of vectors that spans <m>V</m>.
        </p>
      </li>
      <li>
        <p>
          The number of vectors in any basis of <m>V</m> is a constant, called the dimension of <m>V</m>.
        </p>
      </li>
      <li>
        <p>
          The number of vectors in any independent set is always less than or equal to the number of vectors in a spanning set.
        </p>
      </li>
      <li>
        <p>
          In a finite-dimensional vector space, any independent set can be enlarged to a basis,
          and any spanning set can be cut down to a basis by deleting vectors that are in the span of the remaining vectors.
        </p>
      </li>
    </ul>
    Another important aspect of dimension is that it reduces many problems,
    such as determining equality of subspaces, to counting problems.
  </p>

  <theorem xml:id="thm-subspace-dim">
    <statement>
      <p>
        Let <m>U</m> and <m>W</m> be subspaces of a finite-dimensional vector space <m>V</m>.
        <ol>
          <li>
            <p>
              If <m>U\subseteq W</m>, then <m>\dim U\leq \dim W</m>.
            </p>
          </li>
          <li>
            <p>
              If <m>U\subseteq W</m> and <m>\dim U=\dim W</m>, then <m>U=W</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <p>
        <ol>
          <li>
            <p>
              Suppose <m>U\subseteq W</m>, and let <m>B=\{\uu_1,\ldots, \uu_k\}</m>
              be a basis for <m>U</m>. Since <m>B</m> is a basis, it's independent.
              And since <m>B\subseteq U</m> and <m>U\subseteq W</m>, <m>B\subseteq W</m>.
              Thus, <m>B</m> is an independent subset of <m>W</m>,
              and since any basis of <m>W</m> spans <m>W</m>,
              we know that <m>\dim U = k \leq \dim W</m>,
              by <xref ref="theorem-steinitz">Theorem</xref>.
            </p>
          </li>
          <li>
            <p>
              Suppose <m>U\subseteq W</m> and <m>\dim U = \dim W</m>.
              Let <m>B</m> be a basis for <m>U</m>.
              As above, <m>B</m> is an independent subset of <m>W</m>.
              If <m>W\neq U</m>, then there is some <m>\ww\in W</m> with <m>\ww\notin U</m>.
              But <m>U=\spn B</m>, so that would mean that <m>B\cup \{\ww\}</m>
              is a linearly independent set containing <m>\dim U+1</m> vectors.
              This is impossible, since <m>\dim W=\dim U</m>,
              so no independent set can contain more than <m>\dim U</m> vectors.
            </p>
          </li>
        </ol>
      </p>
    </proof>

  </theorem>

  <p>
    An even more useful counting result is the following:
  </p>

  <theorem xml:id="thm-half-the-work">
    <statement>
      <p>
        Let <m>V</m> be an <m>n</m>-dimensional vector space.
        If the set <m>S</m> contains <m>n</m> vectors,
        then <m>S</m> is independent if and only if <m>\spn S=V</m>.
      </p>
    </statement>
    <proof>
      <p>
        If <m>S</m> is independent, then it can be extended to a basis <m>B</m> with <m>S\subseteq B</m>.
        But <m>S</m> and <m>B</m> both contain <m>n</m> vectors (since <m>\dim V=n</m>),
        so we must have <m>S=B</m>.
      </p>

      <p>
        If <m>S</m> spans <m>V</m>, then <m>S</m> must contain a basis <m>B</m>,
        and as above, since <m>S</m> and <m>B</m> contain the same number of vectors,
        they must be equal.
      </p>
    </proof>

  </theorem>

  <paragraphs xml:id="pars-subspace-combine">
    <title>New subspaces from old</title>
    <p>
      On your first assignment, you showed that if <m>U</m> and <m>W</m> are subspaces of a vector space <m>V</m>,
      then the intersection <m>U\cap W</m> is also a subspace of <m>V</m>.
      You also showed that the union <m>U\cup W</m> is generally not a subspace,
      unless one subspace is contained in the other
      (in which case the union is just the larger of the two subspaces we already have).
    </p>

    <p>
      In class, we discussed the fact that the right way to define a subspace containing both <m>U</m> and <m>W</m>
      is using their <term>sum</term>: we define the sum <m>U+W</m> of two subspaces by
      <me>
        U+W = \{\uu+\ww \,|\, \uu\in U \text{ and } \ww\in W\}
      </me>.
      We proved that <m>U+W</m> is again a subspace of <m>V</m>.
    </p>

    <p>
      If <m>U\cap W = \{\mathbf{0}\}</m>, we say that the sum is a <term>direct sum</term>,
      and write it as <m>U\oplus W</m>.
      The following theorem might help us understand why direct sums are singled out for special attention:
    </p>
    <theorem xml:id="thm-sum-dimension">
      <statement>
        <p>
          Let <m>U</m> and <m>W</m> be subspaces of a finite-dimensional vector space <m>V</m>.
          Then <m>U+W</m> is finite-dimensional, and
          <me>
            \dim(U+W)=\dim U + \dim W - \dim(U\cap W)
          </me>.
        </p>
      </statement>

    </theorem>

    <p>
      If the sum is direct, then we have simply <m>\dim(U\oplus W) = \dim U + \dim W</m>.
      The other reason why direct sums are preferable, is that any <m>\vv\in U\oplus W</m>
      can be written <em>uniquely</em> as <m>\vv=\uu+\ww</m>
      where <m>\uu\in U</m> and <m>\ww\in W</m>.
    </p>

    <theorem xml:id="thm-direct-sum">
      <statement>
        <p>
          For any subspaces <m>U,W</m> of a vector space <m>V</m>,
          <m>U\cap W = \{\mathbf{0}\}</m> if and only if for every <m>\vv\in U+W</m>
          there exist unique <m>\uu\in U, \ww\in W</m> such that <m>\vv=\uu+\ww</m>.
        </p>
      </statement>
      <proof>
        <p>
          Suppose that <m>U\cap W = \{\mathbf{0}\}</m>, and suppose that we have
          <m>\vv = \uu_1+\ww_1 = \uu_2+\ww_2</m>,
          for <m>\uu_1,\uu_2\in U,\ww_1,\ww_2\in W</m>.
          Then <m>\mathbf{0}=(\uu_1-\uu_2)+(\ww_1-\ww_2)</m>,
          which implies that
          <me>
            \ww_1-\ww_2 = -(\uu_1-\uu_2)
          </me>.
          Now, <m>\uu=\uu_1-\uu_2\in U</m>,
          since <m>U</m> is a subspace, and similarly,
          <m>\ww=\ww_1-\ww_2\in W</m>.
          But we also have <m>\ww=-\uu</m>, which implies that <m>\ww\in U</m>.
          Therefore, <m>\ww\in U\cap W</m>, which implies that <m>\ww=\mathbf{0}</m>,
          so <m>\ww_1=\ww_2</m>.
          But we must also then have <m>\uu=\mathbf{0}</m>, so <m>\uu_1=\uu_2</m>.
        </p>

        <p>
          Conversely, suppose that every <m>\vv\in U+W</m> can be written uniquely as <m>\vv=vec{u}+\ww</m>,
          with <m>\uu\in U</m> and <m>\ww\in W</m>. Suppose that <m>\mathbf{a}\in U\cap W</m>.
          Then <m>\mathbf{a}\in U</m> and <m>\mathbf{a}\in W</m>, so we also have <m>-\mathbf{a}\in W</m>,
          since <m>W</m> is a subspace.
          But then <m>\mathbf{0}=\mathbf{a}+(-\mathbf{a})</m>, where <m>\mathbf{a}\in U</m> and <m>-\mathbf{a}\in W</m>.
          On the other hand, <m>\mathbf{0}=\mathbf{0}+\mathbf{0}</m>,
          and <m>\mathbf{0}</m> belongs to both <m>U</m> and <m>W</m>. It follows that <m>\mathbf{a}=\mathbf{0}</m>.
          Since <m>\mathbf{a}</m> was arbitrary, <m>U\cap W = \{\mathbf{0}\}</m>.
        </p>
      </proof>

    </theorem>

    <p>
      We end with one last application of the theory we've developed on the existence of a basis for a finite-dimensional vector space.
      As we continue on to later topics, we'll find that it is often useful to be able to decompose a vector space into a direct sum of subspaces.
      Using bases, we can show that this is always possible.
    </p>

    <theorem xml:id="thm-construct-complement">
      <statement>
        <p>
          Let <m>V</m> be a finite-dimensional vector space, and let <m>U</m> be any subspace of <m>V</m>.
          Then there exists a subspace <m>W\subseteq V</m> such that <m>U\oplus W = V</m>.
        </p>
      </statement>
      <proof>
        <p>
          Let <m>\{\uu_1,\ldots, \uu_m\}</m> be a basis of <m>U</m>.
          Since <m>U\subseteq W</m>, the set <m>\{\uu_1,\ldots, \uu_m\}</m>
          is a linearly independent subset of <m>V</m>.
          Since any linearly independent set can be extended to a basis of <m>V</m>,
          there exist vectors <m>\ww_1,\ldots,\ww_n</m> such that
          <me>
            \{\uu_1,\ldots, \uu_m,\ww_1,\ldots, \ww_n\}
          </me>
          is a basis of <m>V</m>.
        </p>

        <p>
          Now, let <m>W = \spn\{\ww_1,\ldots, \ww_n\}</m>.
          Then <m>W</m> is a subspace, and <m>\{\ww_1,\ldots, \ww_n\}</m>
          is a basis for <m>W</m>. (It spans, and must be independent since it's a subset of an independent set.)
        </p>

        <p>
          Clearly, <m>U+W=V</m>, since <m>U+W</m> contains the basis for <m>V</m> we've constructed.
          To show the sum is direct, it suffices to show that <m>U\cap W = \{\mathbf{0}\}</m>.
          To that end, suppose that <m>\vv\in U\cap W</m>.
          Since <m>\vv\in U</m>, we have
          <me>
            \vv=a_1\uu_1+\cdots +a_m\uu_m
          </me>
          for scalars <m>a_1,\ldots, a_m</m>. Since <m>\vv\in W</m>, we can write
          <me>
            \vv=b_1\ww_1+\cdots + b_n\ww_n
          </me>
          for scalars <m>b_1,\ldots, b_n</m>. But then
          <me>
            \mathbf{0}=\vv-\vv=a_1\uu_1+\cdots a_m\uu_m-b_1\ww_1-\cdots -b_n\ww_n.
          </me>
          Since <m>\{\uu_1,\ldots, \uu_m,\ww_1,\ldots, \ww_n\}</m> is a basis for <m>V</m>,
          it's independent, and therefore, all of the <m>a_i,b_j</m> must be zero, and therefore, <m>\vv=\mathbf{0}</m>.

        </p>
      </proof>


    </theorem>

    <p>
      The subspace <m>W</m> constructed in the theorem above is called a <term>complement</term> of <m>U</m>.
      It is not unique; indeed, it depends on the choice of basis vectors.
      For example, if <m>U</m> is a one-dimensional subspace of <m>\R^2</m>; that is, a line,
      then any other non-parallel line through the origin provides a complement of <m>U</m>.
      Later we will see that an especially useful choice of complement is the <term>orthogonal complement</term>.
    </p>
  </paragraphs>
</section>
