<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="sec-kernel-image">
  <title>Kernel and Image</title>

  <p>
    Given any linear transformation <m>T:V\to W</m> we can associate two important subspaces:
    the <term>kernel</term> of <m>T</m> (also known as the <term>nullspace</term>),
    and the <term>image</term> of <m>T</m> (also known as the <term>range</term>).
  </p>

  <definition xml:id="def-kernel-image">
    <statement>
      <p>
        Let <m>T:V\to W</m> be a linear transformation. The <term>kernel</term> of <m>T</m>,
        denoted <m>\ker T</m>, is defined by
        <me>
          \ker T = \{\vv\in V \,|\, T(\vv)=\mathbf{0}\}
        </me>.
        The <term>image</term> of <m>T</m>, denoted <m>\im T</m>, is defined by
        <me>
          \im T = \{T(\vv) \,|\, \vv\in V\}
        </me>.
      </p>
    </statement>
  </definition>

  <p>
    Note that the kernel of <m>T</m> is just the set of all vectors <m>T</m> sends to zero.
    The image of <m>T</m> is the range of <m>T</m> in the usual sense of the range of a function.
  </p>

  <theorem xml:id="thm-ker-img-subspace">
    <statement>
      <p>
        For any linear transformation <m>T:V\to W</m>,
        <ol>
          <li>
            <p>
              <m>\ker T</m> is a subspace of <m>V</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\im T</m> is a subspace of <m>W</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>

    <proof>
      <p>
        <ol>
          <li>
            <p>
              To show that <m>\ker T</m> is a subspace, first note that <m>\mathbf{0}\in \ker T</m>,
              since <m>T(\mathbf{0})=\mathbf{0}</m> for any linear transformation <m>T</m>.
              If <m>\vv,\ww\in \ker T</m>, then <m>T(\vv)=\mathbf{0}</m>
              and <m>T(\ww)=0</m>, and therefore,
              <me>
                T(\vv+\ww)=T(\vv)+T(\ww)=\mathbf{0}+\mathbf{0}=\mathbf{0}
              </me>.
              Similarly, for any scalar <m>c</m> and <m>\vv\in \ker T</m>,
              <me>
                T(c\vv)=cT(\vv)=c\mathbf{0}=\mathbf{0}
              </me>.
              By the subspace test, <m>\ker T</m> is a subspace.
            </p>
          </li>

          <li>
            <p>
              Again, since <m>T(\mathbf{0})=\mathbf{0}</m>, we see that <m>\mathbf{0}\in \im T</m>,
              so <m>\im T</m> is nonempty.
              If <m>\ww_1,\ww_2\in \im T</m>, then there exist <m>\vv_1,\vv_2\in V</m>
              such that <m>T(\vv_1)=\ww_1</m> and <m>T(\vv_2)=\ww_2</m>.
              It follows that
              <me>
                \ww_1+\ww_2 = T(\vv_1)+T(\vv_2) = T(\vv_1+\vv_2)
              </me>,
              so <m>\ww_1+\ww_2\in \im T</m>.
              Similarly, if <m>c</m> is any scalar and <m>\ww=T(\vv)\in\im T</m>,
              then
              <me>
                c\ww=cT(\vv)=T(c\vv)
              </me>,
              so <m>c\ww\in \im T</m>.
            </p>
          </li>
        </ol>
      </p>
    </proof>

  </theorem>

  <p>
    A familiar setting that you may already have encountered in a previous linear algebra course is that of a matrix transformation.
    Let <m>A</m> be an <m>m\times n</m> matrix. Then we can define <m>T:\R^n\to \R^m</m>
    by <m>T(\xx)=A\xx</m>, where elements of <m>\R^n,\R^m</m> are considered as column vectors.
    We then have
    <me>
      \ker T = \nll(A) = \{\xx\in \R^n \,|\, A\xx=\mathbf{0}\}
    </me>
    and
    <me>
      \im T = \csp(A) = \{A\xx\,|\, \xx\in \R^n\}
    </me>,
    where <m>\csp(A)</m> denotes the <term>column space</term> of <m>A</m>.
    Recall further that if we write <m>A</m> in terms of its columns as
    <me>
      A = \bbm C_1 \amp C_2 \amp \cdots \amp C_n\ebm
    </me>
    and a vector <m>\xx\in \R^n</m> as <m>\xx=\bbm x_1\\x_2\\\vdots \\x_n\ebm</m>,
    then
    <me>
      A\xx = x_1C_1+x_2C_2+\cdots +x_nC_n
    </me>.
    Thus, any element of <m>\csp(A)</m> is a linear combination of its columns,
    explaining the name <em>column space</em>.
  </p>

  <p>
    Determining <m>\nll(A)</m> and <m>\csp(A)</m> for a given matrix <m>A</m> is,
    unsurprisingly, a matter of reducing <m>A</m> to row-echelon form.
    Finding <m>\nll(A)</m> is simply a matter of describing the set of all solutions to the homogeneous system <m>A\xx=\mathbf{0}</m>.
    Finding <m>\csp(A)</m> relies on the following theorem.
  </p>

  <theorem xml:id="thm-colspace">
    <statement>
      <p>
        Let <m>A</m> be an <m>m\times n</m> matrix with columns <m>C_1,C_2,\ldots, C_n</m>.
        If the reduced row-echelon form of <m>A</m> has leading ones in columns <m>j_1,j_2,\ldots, j_k</m>,
        then
        <me>
          \{C_{j_1},C_{j_2},\ldots, C_{j_k}\}
        </me>
        is a basis for <m>\csp(A)</m>.
      </p>
    </statement>
  </theorem>

  <p>
    The truth of this theorem is demonstrated in Section 5.4 of the text by Nicholson.
    To see why it works, we need to remember a few basic facts from elementary linear algebra.
    First, recall that performing an elementary row operation on a matrix <m>A</m>
    is equivalent to multiplying on the left by an elementary matrix <m>E</m>
    defined using the same row operation.
  </p>

  <p>
    Since every elementary matrix is invertible, and any product of invertible matrices is invertible,
    and we can transform <m>A</m> into a row-echelon matrix <m>R</m> using elementary row operations,
    it follows that <m>R = UA</m> for an invertible matrix <m>U</m>; indeed,
    we have <m>U = E_kE_{k-1}\cdots E_2E_1</m>, where <m>E_1,\ldots, E_k</m>
    are the elementary matrices corresponding to the row operations used to carry <m>A</m> to <m>R</m>.
  </p>

  <p>
    A basis for <m>\csp(R)</m> is given by the columns of <m>R</m> containing the leading ones.
    The reason for this is as follows. First, recall that each nonzero row begins with a leading one.
    So if the leading ones of <m>R</m> are in columns <m>i_1,\ldots, i_k</m>,
    then there are <m>k</m> nonzero rows.
    Since all rows of zeros go at the bottom, each column in <m>R</m> has its last <m>m-k</m> entries identically zero.
    Thus,
    <me>
      \csp(R)\subseteq \left\{\bbm a_1\\\vdots \\a_k\\0\\\vdots 0\ebm\in \R^m \,|\, a_1,\ldots, a_k\in\R\right\}
    </me>,
    so <m>\dim \csp(R)\leq k</m>. But the columns containing leading ones are easily shown to be independent,
    so they form a basis of <m>\csp(R)</m>, which therefore has dimension <m>k=\operatorname{rank}(A)</m>.
  </p>

  <p>
    Next, since <m>R=UA</m>, where <m>U</m> is invertible, if <m>R=\bbm Y_1\amp Y_2\amp \cdots \amp Y_n\ebm</m>
    and <m>A = \bbm C_1\amp C_2\amp \cdots \amp C_n\ebm</m>, then <m>C_i = U^{-1}Y_i</m> for each <m>i</m>.
    It follows from the fact that <m>U</m> is invertible and that the columns containing leading ones in <m>R</m>
    form a basis for <m>\csp(R)</m> that the corresponding columns in <m>A</m> form a basis for <m>\csp(A)</m>.
    (For details, see Section 5.4 in Nicholson.)
  </p>

  <p>
    For example, consider the linear transformation <m>T:\R^4\to \R^3</m> defined by the matrix
    <me>
      A = \bbm 1 \amp 3 \amp 0 \amp -2\\
              -2 \amp -1 \amp 2 \amp 0\\
               1 \amp 8 \amp 2 \amp -6\ebm
    </me>.
    Let's determine the RREF of <m>A</m>:
  </p>

  <sage>
    <input>
      from sympy import Matrix,init_printing
      init_printing()
      A=Matrix(3,4,[1,3,0,-2,-2,-1,2,0,1,8,2,-6])
      A.rref()
    </input>
    <output>
      \[\bbm 1\amp 0\amp -\frac65 \amp \frac25\\ 0\amp 1\amp \frac25 \amp -\frac45\\0\amp 0\amp 0\amp 0\ebm,(0,1)\]
    </output>
  </sage>

  <p>
    We see that there are leading ones in the first and second column.
    Therefore, <m>\csp(A) = \im(T) = \spn\left\{\bbm 1\\-2\\1\ebm, \bbm 3\\-1\\8\ebm\right\}</m>.
    Indeed, note that
    <me>
      \bbm 0\\2\\2\ebm = -\frac65\bbm 1\\-2\\1\ebm + \frac25\bbm 3\\-1\\8\ebm
    </me>
    and
    <me>
      \bbm -2\\0\\-6\ebm = \frac25\bbm 1\\-2\\1\ebm -\frac45\bbm 3\\-1\\8\ebm
    </me>,
    so that indeed, the third and fourth columns are in the span of the first and second.
  </p>

  <p>
    Furthermore, we can determine the nullspace: if <m>A\xx=\mathbf{0}</m> where
    <m>\xx=\bbm x_1\\x_2\\x_3\\x_4\ebm</m>, then we must have
    <md>
      <mrow>x_1 \amp =\frac65 x_3-\frac25 x_4</mrow>
      <mrow>x_2 \amp =-\frac25 x_3+\frac 45 x_4</mrow>
    </md>,
    so
    <me>
      \xx = \bbm \frac65x_3-\frac25x_4\\ -\frac25x_3+\frac45x_4\\x_3\\x_4\ebm = \frac{x_3}{5}\bbm 6\\-2\\5\\0\ebm + \frac{x_4}{5}\bbm -2\\4\\0\\5\ebm
    </me>.
    It follows that a basis for <m>\nll(A)=\ker T</m> is <m>\left\{\bbm 6\\-2\\5\\0\ebm, \bbm -2\\4\\0\\5\ebm\right\}</m>.
  </p>

  <p>
    Incidentally, the SymPy library for Python has built-in functions for computing nullspace and column space.
    But it's probably worth your while to know how to determine these from the <init>RREF</init> of a matrix,
    without additional help from the computer.
    That said, let's see how the computer's output compares to what we found:
  </p>

  <sage>
    <input>
      A.nullspace()
    </input>
    <output>
      \[\left[\bbm \frac65\\-\frac25\\1\\0\ebm, \bbm -\frac25\\ \frac45\\0\\1\ebm\right]\]
    </output>
  </sage>

  <sage>
    <input>
      A.columnspace()
    </input>
    <output>
      \[\left[\bbm 1\\-2\\1\ebm, \bbm 3\\-1\\8\ebm\right]\]
    </output>
  </sage>

  <p>
    Note that the output from the computer simply states the basis for each space.
    Of course, for computational purposes, this is typically good enough.
  </p>

  <p>
    An important result that comes out while trying to show that the <q>pivot columns</q>
    of a matrix (the ones that end up with leading ones in the RREF) are a basis for the column space
    is that the column rank (defined as the dimension of <m>\csp(A)</m>) and the row rank
    (the dimension of the space spanned by the columns of <m>A</m>) are equal.
    One can therefore speak unambiguously about the <term>rank</term> of a matrix <m>A</m>,
    and it is just as it's defined in a first course in linear algebra: the number of leading ones in the RREF of <m>A</m>.
  </p>

  <p>
    For a general linear transformation, we can't necessarily speak in terms of rows and columns,
    but if <m>T:V\to W</m> is linear, and either <m>V</m> or <m>W</m> is finite-dimensional,
    then we can define the rank of <m>T</m> as follows.
  </p>

  <definition xml:id="def-rank-transformation">
    <statement>
      <p>
        Let <m>T:V\to W</m> be a linear transformation.
        Then the <term>rank</term> of <m>T</m> is defined by
        <me>
          \operatorname{rank} T = \dim \im T
        </me>,
        and the <term>nullity</term> of <m>T</m> is defined by
        <me>
          \operatorname{nullity} T = \dim \ker T
        </me>.

      </p>
    </statement>
  </definition>

  <p>
    Note that if <m>W</m> is finite-dimensional, then so is <m>\im T</m>,
    since it's a subspace of <m>W</m>.
    On the other hand, if <m>V</m> is finite-dimensional,
    then we can find a basis <m>\{\vv_1,\ldots, \vv_n\}</m> of <m>V</m>,
    and the set <m>\{T(\vv_1),\ldots, T(\vv_n)\}</m> will span <m>\im T</m>,
    so again the image is finite-dimensional, so the rank of <m>T</m> is finite.
    It is possible for either the rank or the nullity of a transformation to be infinite.
  </p>

  <p>
    Knowing that the kernel and image of an operator are subspaces gives us an easy way to define subspaces.
    From the textbook, we have the following nice example.
  </p>

  <example>
    <statement>
      <p>
        Let <m>T:M_{nn}\to M_{nn}</m> be defined by <m>T(A)=A-A^T</m>. Then
        <ol>
          <li>
            <p>
              <m>T</m> is a linear map.
            </p>
          </li>
          <li>
            <p>
              <m>\ker T</m> is equal to the set of all symmetric matrices.
            </p>
          </li>

          <li>
            <p>
              <m>\im T</m> is equal to the set of all skew-symmetric matrices.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <solution>
      <p>
        <ol>
          <li>
            <p>
              We have <m>T(0)=0</m> since <m>0^T=0</m>.
              Using proerties of the transpose and matrix algebra, we have
              <me>
                T(A+B) = (A+B)-(A+B)^T = (A-A^T)+(B-B^T) = T(A)+T(B)
              </me>
              and
              <me>
                T(kA) = (kA) - (kA)^T = kA-kA^T = k(A-A^T) = kT(A)
              </me>.
            </p>
          </li>

          <li>
            <p>
              It's clear that if <m>A^T=A</m>, then <m>T(A)=0</m>.
              On the other hand, if <m>T(A)=0</m>, then <m>A-A^T=0</m>, so <m>A=A^T</m>.
              Thus, the kernel consists of all symmetric matrices.
            </p>
          </li>

          <li>
            <p>
              If <m>B=T(A)=A-A^T</m>, then
              <me>
                B^T = (A-A^T)^T = A^T-A = -B
              </me>,
              so certainly every matrix in <m>\im A</m> is skew-symmetric.
              On the other hand, if <m>B</m> is skew-symmetric, then <m>B=T(\frac12 B)</m>,
              since
              <me>
                T\Bigl(\frac12 B\Bigr) = \frac12 T(B) = \frac12(B-B^T) = \frac12(B-(-B))= B
              </me>.
            </p>
          </li>
        </ol>
      </p>
    </solution>
  </example>

  <p>
    You'll recall from a course like Math 2000 that in the study of functions,
    the properties of being injective (one-to-one) and surjective (onto)
    are important. They're important for linear transformations as well,
    and defined in exactly the same way.
  </p>

  <p>
    It's clear that being surjective is closely tied to image.
    Indeed, by definition, <m>T:V\to W</m> is onto if <m>\im T = W</m>.
    What might not be immediately obvious is that the kernel tells us if a linear map is injective.
  </p>

  <theorem xml:id="thm-injective-kernel">
    <statement>
      <p>
        Let <m>T:V\to W</m> be a linear transformation.
        Then <m>T</m> is injective if and only if <m>\ker T = \{\mathbf{0}\}</m>.
      </p>
    </statement>

    <proof>
      <p>
        Suppose <m>T</m> is injective, and let <m>\vv\in \ker T</m>.
        Then <m>T(\vv)=\mathbf{0}</m>. On the other hand, we know that <m>T(\mathbf{0})=\mathbf{0}</m>,
        and since <m>T</m> is injective, we must have <m>\vv=\mathbf{0}</m>.

        Conversely, suppose that <m>\ker T = \{0\}</m> and that <m>T(\vv_1)=T(\vv_2)</m>
        for some <m>\vv_1,\vv_2\in V</m>. Then
        <me>
          \mathbf{0} = T(\vv_1)-T(\vv_2) = T(\vv_1-\vv_2)
        </me>,
        so <m>\vv_1-\vv_2\in \ker T</m>.
        Therefore, we must have <m>\vv_1-\vv_2=\mathbf{0}</m>,
        so <m>\vv_1=\vv_2</m>, and it follows that <m>T</m> is injective.
      </p>
    </proof>

  </theorem>

  <p>
    Let us return to the case of a matrix transformation <m>T_A:\R^n\to \R^m</m>.
    Notice that <m>\ker T_A</m> is simply the set of all solutions to <m>A\xx=\mathbf{0}</m>,
    while <m>\im T_A</m> is the set of all <m>\yy\in\R^m</m> for which <m>A\xx=\yy</m>
    <em>has</em> a solution.
  </p>

  <p>
    Recall from the discussion above that <m>\rank A = \dim \csp(A) = \dim \im T_A</m>.
    It follows that <m>T_A</m> is surjective if and only if <m>\rank A = m</m>.
    On the other hand, <m>T_A</m> is injective if and only if <m>\rank A = n</m>,
    because we know that the system <m>A\xx=\mathbf{0}</m> has a unique solution if and only if each column of <m>A</m> contains a leading one.
  </p>

  <p>
    This has some interesting consequences. If <m>m=n</m> (that is, if <m>A</m> is square),
    then each increase in <m>\dim \nll(A)</m> produces a corresponding decrease in <m>\dim \csp(A)</m>,
    since both correspond to the <q>loss</q> of a leading one. Moreover, if <m>\rank A = n</m>,
    then <m>T_A</m> is both injective and surjective.
    Recall that a function is invertible if and only if it is both injective and surjective.
    It should come as no surprise that invertibility of <m>T_A</m> (as a function)
    is equivalent to invertibility of <m>A</m> (as a matrix).
  </p>

  <p>
    Also, note that if <m>m \lt n</m>, then <m>\rank A\leq m \lt n</m>,
    so <m>T_A</m> could be surjective, but can't possibly be injective.
    On the other hand, if <m>m\gt n</m>, then <m>\rank A\leq n \lt m</m>,
    so <m>T_A</m> could be injective, but can't possibly be surjective.
    These results generalize to linear transformations between any finite-dimensional vector spaces.
    The first step towards this is the following theorem,
    which is sometimes known as the Fundamental Theorem of Linear Transformations.
  </p>

  <theorem xml:id="thm-dimension-lintrans">
    <title>Dimension Theorem</title>
    <statement>
      <p>
        Let <m>T:V\to W</m> be any linear transformation such that
        <m>\ker T</m> and <m>\im T</m> are finite-dimensional.
        Then <m>V</m> is finite-dimensional, and
        <me>
          \dim V = \dim \ker T + \dim \im T
        </me>.
      </p>
    </statement>
    <proof>
      <p>
        The trick with this proof is that we aren't assuming <m>V</m> is finite-dimensional,
        so we can't start with a basis of <m>V</m>. But we do know that <m>\im T</m>
        is finite-dimensional, so we start with a basis <m>\{\ww_1,\ldots, \ww_m\}</m>
        of <m>\im T</m>.
        Of course, every vector in <m>\im T</m> is the image of some vector in <m>V</m>,
        so we can write <m>\ww_i =T(\vv_i)</m>, where <m>\vv_i\in V</m>,
        for <m>i=1,2,\ldots, m</m>.
      </p>

      <p>
        Since <m>\{T(\vv_1),\ldots, T(\vv_m)\}</m> is a basis,
        it is linearly independent. The results of <xref ref="ex_lintrans-indep">Exercise</xref>
        tell us that the set <m>\{\vv_1,\ldots, \vv_m\}</m> must therefore be independent.
      </p>

      <p>
        We now introduce a basis <m>\{\uu_1,\ldots, \uu_n\}</m>
        of <m>\ker T</m>, which we also know to be finite-dimensional.
        If we can show that the set <m>\{\uu_1,\ldots, \uu_n,\vv_1,\ldots, \vv_m\}</m>
        is a basis for <m>V</m>, we'd be done, since the number of vectors in this basis
        is <m>\dim\ker T + \dim \im T</m>. We must therefore show that this set is independent,
        and that it spans <m>V</m>.
      </p>

      <p>
        To see that it's independent, suppose that
        <me>
          a_1\uu_1+\cdots + a_n\uu_n+b_1\vv_1+\cdots +b_m\vv_m=\mathbf{0}
        </me>.
        Applying <m>T</m> to this equation, and noting that <m>T(\uu_i)=\mathbf{0}</m>
        for each <m>i</m>, by definition of the <m>\uu_i</m>, we get
        <me>
          b_1T(\vv_1)+\cdots +b_mT(\vv_m)=\mathbf{0}
        </me>.
        We assumed that the vectors <m>T(\vv_i)</m> were independent,
        so all the <m>b_i</m> must be zero. But then we get
        <me>
          a_1\uu_1+\cdots +a_n\uu_n=\mathbf{0}
        </me>,
        and since the <m>\uu_i</m> are independent, all the <m>a_i</m> must be zero.
      </p>

      <p>
        To see that these vectors span, choose any <m>\xx\in V</m>.
        Since <m>T(\xx)\in \im T</m>, there exist scalars <m>c_1,\ldots, c_m</m>
        such that
        <men xml:id="eqn-almost-span">
          T(\xx)=c_1T(\vv_1)+\cdots +c_mT(\vv_m)
        </men>.
        We'd like to be able to conclude from this that <m>\xx=c_1\vv_1+\cdots +c_m\vv_m</m>,
        but this would be false, unless <m>T</m> was known to be injective (which it isn't).
        Failure to be injective involves the kernel -- how do we bring that into the picture?
      </p>

      <p>
        The trick is to realize that the reason we might have <m>\xx\neq c_1\vv_1+\cdots +c_m\vv_m</m>
        is that we're off by something in the kernel.
        Indeed, <xref ref="eqn-almost-span"/> can be re-written as
        <me>
          T(\xx-c_1\vv_1-\cdots -c_m\vv_m) = \mathbf{0}
        </me>,
        so <m>\xx-c_1\vv_1-\cdots -c_m\vv_m\in\ker T</m>.
        But we have a basis for <m>\ker T</m>, so we can write
        <me>
          \xx-c_1\vv_1-\cdots -c_m\vv_m=t_1\uu_1+\cdots +t_n\uu_n
        </me>
        for some scalars <m>t_1,\ldots, t_n</m>, and this can be rearanged to give
        <me>
          \xx=t_1\uu_1+\cdots +t_n\uu_n+c_1\vv_1+\cdots + c_m\vv_m
        </me>,
        which completes the proof.
      </p>
    </proof>

  </theorem>

  <p>
    This is sometimes known as the <em>Rank-Nullity Theorem</em>,
    since it can be stated in the form
    <me>
      \dim V = \rank T + \operatorname{nullity} T
    </me>.
    We will see that this result is frequently useful for providing simple arguments that establish otherwise difficult results.
    A basic situation where the theorem is useful is as follows:
    we are given <m>T:V\to W</m>, where the dimensions of <m>V</m> and <m>W</m> are known.
    Since <m>\im T</m> is a subspace of <m>W</m>, we know from <xref ref="thm-subspace-dim">Theorem</xref>
    that <m>T</m> is onto if and only if <m>\dim \im T = \dim W</m>.
    In many cases it is easier to compute <m>\ker T</m> than it is <m>\im T</m>,
    and the Dimension Theorem lets us determine <m>\dim\im T</m> if we know <m>\dim V</m> and <m>\dim \ker T</m>.
  </p>

  <p>
    A useful consequence of this result is that if we know <m>V</m> is finite-dimensional,
    we can order any basis such that the first vectors in the list are a basis of <m>\ker T</m>,
    and the images of the remaining vectors produce a basis of <m>\im T</m>.
  </p>

  <p>
    Note that one consequence of the dimension theorem is that we must have
    <me>
      \dim \ker T\leq \dim V \quad \text{ and } \quad \dim \im T\leq \dim V
    </me>.
    Of course, we must also have <m>\dim\im T\leq \dim W</m>,
    since <m>\im T</m> is a subspace of <m>W</m>.
    In the case of a matrix transformation <m>T_A</m>,
    this means that the rank of <m>T_A</m> is at most the minimum of <m>\dim V</m> and <m>\dim W</m>.
    This once again has consequences for the existence and uniqueness of solutions for linear systems with the coefficient matrix <m>A</m>.
  </p>

  <exercise xml:id="ex-dimension-injection-surjection">
    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be finite-dimensional vector spaces. Prove the following:
        <ol>
          <li>
            <p>
              <m>\dim V\leq \dim W</m> if and only if there exists an injection <m>T:V\to W</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\dim V\geq \dim W</m> if and only if there exists a surjection <m>T:V\to W</m>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <solution>
      <p>
        <ol>
          <li>
            <p>
              Suppose <m>T:V\to W</m> is injective. Then <m>\ker T = \{0\}</m>, so
              <me>
                \dim V = 0 + \dim \im T \leq \dim W
              </me>,
              since <m>\im T</m> is a subspace of <m>W</m>.
            </p>

            <p>
              Conversely, suppose <m>\dim V\leq \dim W</m>.
              Choose a basis <m>\{\vv_1,\ldots, \vv_m\}</m> of <m>V</m>,
              and a basis <m>\{\ww_1,\ldots, \ww_n\}</m> of <m>W</m>, where <m>m\leq n</m>.
              By <xref ref="thm-define-using-basis">Theorem</xref>, there exists a linear transformation
              <m>T:V\to W</m> with <m>T(\vv_i)=\ww_i</m> for <m>i=1,\ldots, m</m>.
              (The main point here is that we run out of basis vectors for <m>V</m> before we run out of basis vectors for <m>W</m>.)
              This map is injective: if <m>T(\vv)=\mathbf{0}</m>,
              write <m>\vv=c_1\vv_1+\cdots + c_m\vv_m</m>.
              Then
              <md>
                <mrow>\mathbf{0} \amp = T(\vv)</mrow>
                <mrow> \amp = T(c_1\vv_1+\cdots + c_m\vv_m)</mrow>
                <mrow>  \amp = c_1T(\vv_1)+\cdots + c_mT(\vv_m)</mrow>
                <mrow>  \amp = c_1\ww_1+\cdots +c_m\ww_m</mrow>
              </md>.
              Since <m>\{\ww_1,\ldots, \ww_m\}</m> is a subset of a basis, it's independent.
              Therefore, the scalars <m>c_i</m> must all be zero, and therefore <m>\vv=\mathbf{0}</m>.
            </p>
          </li>

          <li>
            <p>
              Suppose <m>T:V\to W</m> is surjective. Then <m>\dim \im T = \dim W</m>, so
              <me>
                \dim V = \dim \ker T + \dim W \geq  \dim W
              </me>.
              Conversely, suppose <m>\dim V\geq \dim W</m>. Again,
              choose a basis <m>\{\vv_1,\ldots, \vv_m\}</m> of <m>V</m>,
              and a basis <m>\{\ww_1,\ldots, \ww_n\}</m> of <m>W</m>,
              where this time, <m>m\geq n</m>.
              We can define a linear transformation as follows:
              <me>
                T(\vv_1)=\ww_1,\ldots, T(\vv_n)=\ww_n, \text{ and } T(\vv_j) = \mathbf{0} \text{ for } j>n.
              </me>
              It's easy to check that this map is a surjection:
              given <m>\ww\in W</m>, we can write it in terms of our basis as <m>\ww=c_1\ww_1+\cdots + c_n\ww_n</m>.
              Using these same scalars, we can define <m>\vv=c_1\vv_1+\cdots + c_n\vv_n\in V</m> such that <m>T(\vv)=\ww</m>.
            </p>

            <p>
              Note that it's not important how we define <m>T(\vv_j)</m> when <m>j>n</m>.
              The point is that this time, we run out of basis vectors for <m>W</m> before we run out of basis vectors for <m>V</m>.
              Once each vector in the basis of <m>W</m> is in the image of <m>T</m>, we're guaranteed that <m>T</m> is surjective,
              and we can define the value of <m>T</m> on any remaining basis vectors however we want.
            </p>
          </li>
        </ol>
      </p>
    </solution>
  </exercise>
</section>
