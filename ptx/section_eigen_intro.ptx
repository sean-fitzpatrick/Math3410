<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="sec-eigen-basics">
  <title>Eigenvalues and Eigenvectors</title>
  <definition xml:id="def-eigenvalue">
    <statement>
      <p>
        Let <m>A</m> be an <m>n\times n</m> matrix.
        A number <m>\lambda</m> is called an <term>eigenvalue</term> of <m>A</m>
        if there exists a nonzero vector <m>\xx</m> such that
        <me>
          A\xx = \lambda\xx
        </me>.
        Any such vector <m>\xx</m> is called an <term>eigenvector</term>
        associated to the eigenvalue <m>\lambda</m>.
      </p>
    </statement>
  </definition>

  <remark>
    <p>
      You might reasonably wonder: where does this definition come from?
      And why should I care?
      We are assuming that you saw at least a basic introduction to eigenvalues
      in your first course on linear algebra, but that course probably focused on mechanics.
      Possibly you learned that diagonalizing a matrix lets you compute powers of that matrix.
    </p>

    <p>
      But why should we be interested in computing powers
      (in particular, <em>large</em> powers) of a matrix?
      An important context comes from the study of
      <url href="https://en.wikipedia.org/wiki/Linear_dynamical_system" visual="en.wikipedia.org/wiki/Linear_dynamical_system">discrete linear dynamical systems</url>,
      as well as <url href="https://en.wikipedia.org/wiki/Markov_chain" visual="en.wikipedia.org/wiki/Markov_chain">Markov chains</url>,
      where the evolution of a state is modelled by repeated multiplication of a vector by a matrix.
    </p>

    <p>
      When we're able to diagonalize our matrix using eigenvalues and eigenvectors,
      not only does it become easy to compute powers of a matrix,
      it also enables us to see that the entire process is just a linear combination of geometric sequences!
      If you have completed <xref ref="worksheet-recurrence"/>,
      you probably will not be surprised to learn that the polynomial roots you found are,
      in fact, eigenvalues of a suitable matrix.
      <!-- We will have more to say about this in <xref ref="worksheet-dynamical"/>. -->
    </p>
  </remark>

  <remark>
    <statement>
      <p>
        Eigenvalues and eigenvectors can just as easily be defined for a general linear operator <m>T:V\to V</m>.
        In this context, and eigenvector <m>\xx</m> is sometimes referred to as a <em>characteristic vector</em> (or characteristic direction)
        for <m>T</m>, since the property <m>T(\xx)=\lambda \xx</m>
        simply states that the transformed vector <m>T(\xx)</m> is parallel to the original vector <m>\xx</m>.
        Some linear algebra textbooks that focus more on general linear transformations frame this topic in the context of
        <em>invariant subspaces</em> for a linear operator.
      </p>

      <p>
        A subspace <m>U\subseteq V</m> is <em>invariant</em> with respect to <m>T</m> if <m>T(\uu)\in U</m> for all <m>\uu\in U</m>.
        Note that if <m>\xx</m> is an eigenvector of <m>T</m>, then <m>\spn\{\xx\}</m> is an invariant subspace.
        To see this, note that if <m>T(\xx)=\lambda \xx</m> and <m>\yy=k\xx</m>, then
        <me>
          T(\yy)=T(k\xx)=kT(\xx)=k(\lambda \xx)=\lambda(k\xx)=\lambda\yy
        </me>.
      </p>
    </statement>
  </remark>

  <exercise label="ex-match-eigen1">
    <statement>
      <p>
        For the matrix <m>A = \bbm -1\amp 0\amp 3\\1\amp -1\amp 0\\1\amp 0\amp 1\ebm</m>,
        match each vector on the left with the corresponding eigenvalue on the right.
        (For typographical reasons, column vectors have been transposed.)
      </p>
    </statement>
    <matches>
      <match order="3">
        <premise><m>\bbm -3\amp 3\amp 1\ebm^T</m></premise>
        <response><m>-2</m></response>
      </match>
      <match order="1">
        <premise><m>\bbm 0\amp 1\amp 0\ebm^T</m></premise>
        <response><m>-1</m></response>
      </match>
      <match order="4">
        <premise><m>\bbm 3\amp 1\amp 3\ebm^T</m></premise>
        <response><m>2</m></response>
      </match>
      <match order="2">
        <premise><m>\bbm 1\amp 1\amp 1\ebm^T</m></premise>
        <response>Not an eigenvector</response>
      </match>
    </matches>
    <hint>
      <p>
        Use <xref ref="def-eigenvalue"/>.
      </p>
    </hint>
  </exercise>

  <p>
    Note that if <m>\xx</m> is an eigenvector of the matrix <m>A</m>, then we have
    <men xml:id="eq-eigen-null">
      (A-\lambda I_n)\xx=\zer
    </men>,
    where <m>I_n</m> denotes the <m>n\times n</m> identity matrix.
    Thus, if <m>\lambda</m> is an eigenvalue of <m>A</m>,
    any corresponding eigenvector is an element of <m>\nll(A-\lambda I_n)</m>.
  </p>

  <definition xml:id="def-eigenspace">
    <statement>
      <p>
        For any real number <m>\lambda</m> and <m>n\times n</m> matrix <m>A</m>,
        we define the <term>eigenspace</term> <m>E_\lambda(A)</m> by
        <me>
          E_\lambda(A) = \nll (A-\lambda I_n)
        </me>.
      </p>
    </statement>
  </definition>

  <p>
    Since we know that the null space of any matrix is a subspace,
    it follows that eigenspaces are subspaces of <m>\R^n</m>.
  </p>

  <p>
    Note that <m>E_\lambda(A)</m> can be defined for any real number <m>\lambda</m>,
    whether or not <m>\lambda</m> is an eigenvalue.
    However, the eigenvalues of <m>A</m> are distinguished by the property that there is a
    <em>nonzero</em> solution to <xref ref="eq-eigen-null"/>.
    Furthermore, we know that <xref ref="eq-eigen-null"/> can only have nontrivial solutions if the matrix <m>A-\lambda I_n</m>
    is not invertible. We also know that <m>A-\lambda I_n</m> is non-invertible if and only if <m>\det (A-\lambda I_n) = 0</m>.
    This gives us the following theorem.
  </p>

  <theorem xml:id="thm-eigenspace-nonzero">
    <statement>
      <p>
        The following are equivalent for any <m>n\times n</m> matrix <m>A</m> and real number <m>\lambda</m>:
        <ol>
          <li>
            <p>
              <m>\lambda</m> is an eigenvalue of <m>A</m>.
            </p>
          </li>
          <li>
            <m>E_\lambda(A)\neq \{\zer\}</m>
          </li>

          <li>
            <m>\det(A-\lambda I_n) = 0</m>
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <title>Strategy</title>
      <p>
        To prove a theorem involving a <q>the following are equivalent</q> statement,
        a good strategy is to show that the first implies the second,
        the second implies the third, and the third implies the first.
        The ideas needed for the proof are given in the paragraph preceding the theorem.
        See if you can turn them into a formal proof.
      </p>
    </proof>
  </theorem>

  <p>
    The polynomial <m>p_A(x)=\det(xI_n -A)</m> is called the <term>characteristic polynomial</term> of <m>A</m>.
    (Note that <m>\det(x I_n-A) = (-1)^n\det(A-x I_n)</m>. We choose this order so that the coefficient of <m>x^n</m> is always 1.)
    The equation
    <men xml:id="eq-characteristic">
      \det(xI_n - A) = 0
    </men>
    is called the <term>characteristic equation</term> of <m>A</m>.
    The solutions to this equation are precisely the eigenvalues of <m>A</m>.
  </p>

  <exercise label="ex-match-eigen2">
    <statement>
      <p>
        In order for certain properties of a matrix <m>A</m> to be satisfied,
        the eigenvalues of <m>A</m> need to have particular values.
        Match each property of a matrix <m>A</m> on the left with the corresponding information about the eigenvalues of <m>A</m> on the right.
        Be sure that you can justify your answers with a suitable proof.
      </p>
    </statement>
    <matches randomize="yes">
      <match order="4">
        <premise><m>A</m> is invertible</premise>
        <response><m>0</m> is not an eigenvalue of <m>A</m></response>
      </match>
      <match order="1">
        <premise><m>A^k=0</m> for some integar <m>k\geq 2</m></premise>
        <response><m>0</m> is the only eigenvalue of <m>A</m></response>
      </match>
      <match order="3">
        <premise><m>A=A^{-1}</m></premise>
        <response><m>1</m> and <m>-1</m> are the only eigenvalues of <m>A</m></response>
      </match>
      <match order="5">
        <premise><m>A^2=A</m></premise>
        <response><m>0</m> and <m>1</m> are the only eigenvalues of <m>A</m></response>
      </match>
      <match order="2">
        <premise><m>A^3=A</m></premise>
        <response><m>0</m>, <m>1</m>, and <m>-1</m> are the eigenvalues of <m>A</m></response>
      </match>
    </matches>
  </exercise>

  <p>
    Recall that a matrix <m>B</m> is said to be <term>similar</term> to a matrix <m>A</m>
    if there exists an invertible matrix <m>P</m> such that <m>B = P^{-1}AP</m>.
    Much of what follows concerns the question of whether or not a given <m>n\times n</m>
    matrix <m>A</m> is <term>diagonalizable</term>.
  </p>

  <definition xml:id="def-diagonalizable">
    <statement>
      <p>
        An <m>n\times n</m> matrix <m>A</m> is said to be <term>diagonalizable</term>
        if <m>A</m> is similar to a diagonal matrix.
      </p>
    </statement>
  </definition>

  <p>
    The following results will frequently be useful.
  </p>

  <theorem xml:id="thm-similar-properties">
    <statement>
      <p>
        The relation <m>A\sim B</m> if and only if <m>A</m> is similar to <m>B</m> is an equivalence relation.
        Moreover, if <m>A\sim B</m>, then:
        <ul>
          <li>
            <m>\det A = \det B</m>
          </li>
          <li>
            <m>\tr A = \tr B</m>
          </li>

          <li>
            <m>c_A(x) = c_B(x)</m>
          </li>
        </ul>
        In other words, <m>A</m> and <m>B</m> have the same determinant, trace, and characteristic polynomial
        (and thus, the same eigenvalues).
      </p>
    </statement>
    <proof>
      <p>
        The first two follow directly from properties of the determinant and trace.
        For the last, note that if <m>B = P^{-1}AP</m>, then
        <me>
          P^{-1}(xI_n-A)P = P^{-1}(xI_n)P-P^{-1}AP = xI_n B
        </me>,
        so <m>xI_n-B\sim xI_n-A</m>, and therefore <m>\det(xI_n-B)=\det(xI_n-A)</m>.
      </p>
    </proof>
  </theorem>



  <example xml:id="example-eigenvectors">
    <statement>
      <p>
        Determine the eigenvalues and eigenvectors of <m>A = \bbm 0\amp 1\amp 1\\1\amp 0\amp 1\\1\amp 1\amp 0\ebm</m>.
      </p>
    </statement>
    <solution>
      <p>
        We begin with the characteristic polynomial. We have
        <md>
          <mrow>\det(xI_n - A) \amp =\det\bbm x \amp -1\amp -1\\-1\amp x \amp -1\\-1\amp -1\amp x\ebm</mrow>
          <mrow> \amp = x \begin{vmatrix}x \amp -1\\-1\amp x\end{vmatrix}
             +1\begin{vmatrix}-1\amp -1\\-1\amp x\end{vmatrix}
             -1\begin{vmatrix}-1\amp x\\-1\amp -1\end{vmatrix}</mrow>
          <mrow> \amp = x(x^2-1)+(-x-1)-(1+x)</mrow>
          <mrow> \amp x(x-1)(x+1)-2(x+1)</mrow>
          <mrow> \amp (x+1)[x^2-x-2]</mrow>
          <mrow> \amp (x+1)^2(x-2)</mrow>
        </md>.
      </p>

      <p>
        The roots of the characteristic polynomial are our eigenvalues,
        so we have <m>\lambda_1=-1</m> and <m>\lambda_2=2</m>.
        Note that the first eigenvalue comes from a repeated root.
        This is typically where things get interesting.
        If an eigenvalue does not come from a repeated root,
        then there will only be one (independent) eigenvector that corresponds to it.
        (That is, <m>\dim E_\lambda(A)=1</m>.)
        If an eigenvalue is repeated, it could have more than one eigenvector,
        but this is not guaranteed.
      </p>

      <p>
        We find that <m>A-(-1)I_n = \bbm 1\amp 1\amp 1\\1\amp 1\amp 1\\1\amp 1\amp 1\ebm</m>,
        which has reduced row-echelon form <m>\bbm 1\amp 1\amp 1\\0\amp 0\amp 0\\0\amp 0\amp 0\ebm</m>.
        Solving for the nullspace, we find that there are two independent eigenvectors:
        <me>
          \xx_{1,1}=\bbm 1\\-1\\0\ebm, \quad \text{ and } \quad \xx_{1,2}=\bbm 1\\0\\-1\ebm
        </me>,
        so
        <me>
          E_{-1}(A) = \spn\left\{\bbm 1\\-1\\0\ebm, \bbm 1\\0\\-1\ebm\right\}
        </me>.
      </p>

      <p>
        For the second eigenvector, we have <m>A-2I = \bbm -2\amp 1\amp 1\\1\amp -2\amp 1\\1\amp 1\amp -2\ebm</m>,
        which has reduced row-echelon form <m>\bbm 1\amp 0\amp -1\\0\amp 1\amp -1\\0\amp 0\amp 0\ebm</m>.
        An eigenvector in this case is given by
        <me>
          \xx_2 = \bbm 1\\1\\1\ebm
        </me>.
      </p>
    </solution>
  </example>

  <p>
    In general, if the characteristic polynomial can be factored as
    <me>
      p_A(x)=(x-\lambda)^mq(x)
    </me>,
    where <m>q(x)</m> is not divisible by <m>x-\lambda</m>, then we say that <m>\lambda</m>
    is an eigenvalue of <term>multiplicity</term> <m>m</m>.
    In the example above, <m>\lambda_1=-1</m> has multiplicty 2,
    and <m>\lambda_2=2</m> has multiplicty 1.
  </p>

  <p>
    The <c>eigenvects</c> command in SymPy takes a square matrix as input,
    and outputs a list of lists (one list for each eigenvalue).
    For a given eigenvalue, the corresponding list has the form
    <c>(eigenvalue, multiplicity, eigenvectors)</c>.
    Using SymPy to solve <xref ref="example-eigenvectors"/> looks as follows:
  </p>

  <sage>
    <input>
      from sympy import Matrix, init_printing
      init_printing()
      A = Matrix([[0,1,1],[1,0,1],1,1,0])
      A.eigenvects()
    </input>
    <output>
      \[\left[\left(-1,2,\left[\bbm -1\\1\\0\ebm,\bbm -1\\0\\1\ebm\right]\right),\left(2,2,\left[\bbm 1\\1\\1\ebm\right]\right)\right]\]
    </output>
  </sage>

  <p>
    An important result about multiplicity is the following.
  </p>

  <theorem xml:id="thm-multiplicity">
    <statement>
      <p>
        Let <m>\lambda</m> be an eigenvalue of <m>A</m> of multiplicity <m>m</m>.
        Then <m>\dim E_\lambda(A)\leq m</m>.
      </p>
    </statement>
  </theorem>

  <aside>
    <p>
      Some textbooks refer to the multiplicity <m>m</m> of an eigenvalue as the
      <em>algebraic multiplicity</em> of <m>\lambda</m>, and the number <m>\dim E_\lambda(A)</m>
      as the <em>geometric multiplicity</em> of <m>\lambda</m>.
    </p>
  </aside>

  <p>
    To prove <xref ref="thm-multiplicity"/> we need the following lemma,
    which we've borrowed from Section 5.5 of Nicholson's textbook.
  </p>

  <lemma xml:id="lem-block-eigen">
    <statement>
      <p>
        Let <m>\{\xx_1,\ldots, \xx_k\}</m> be a set of linearly independent eigenvectors of a matrix <m>A</m>,
        with corresponding eigenvalues <m>\lambda_1,\ldots, \lambda_k</m> (not necessarily distinct).
        Extend this set to a basis <m>\{\xx_1,\ldots, \xx_k,\xx_{k+1},\ldots, \xx_n\}</m>,
        and let <m>P=\bbm \xx_1\amp \cdots \amp \xx_n\ebm</m>
        be the matrix whose columns are the basis vectors. (Note that <m>P</m> is necessarily invertible.)
        Then
        <me>
          P^{-1}AP = \bbm \diag(\lambda_1,\ldots, \lambda_k) \amp B\\0\amp A_1\ebm
        </me>,
        where <m>B</m> has size <m>k\times (n-k)</m>, and <m>A_1</m> has size <m>(n-k)\times (n-k)</m>.
      </p>
    </statement>
    <proof>
      <p>
        We have
        <md>
          <mrow>P^{-1}AP \amp = P^{-1}A\bbm \xx_1\amp \cdots \amp \xx_n\ebm</mrow>
          <mrow> \amp =\bbm (P^{-1}A)\xx_1\amp \cdots \amp (P^{-1}A)\xx_n\ebm</mrow>
        </md>.
        For <m>1\leq i\leq k</m>, we have
        <me>
          (P^{-1}A)(\xx_i) = P^{-1}(A\xx_i) = P^{-1}(\lambda_i\xx_i)=\lambda_i(P^{-1}\xx_i)
        </me>.
        But <m>P^{-1}\xx_i</m> is the <m>i</m>th column of <m>P^{-1}P = I_n</m>,
        which proves the result.
      </p>
    </proof>
  </lemma>

  <p>
    We can use <xref ref="lem-block-eigen"/> to prove that <m>\dim E_\lambda(A)\leq m</m> as follows.
    Suppose <m>\{\xx_1,\ldots, \xx_k\}</m> is a basis for <m>E_\lambda(A)</m>.
    Then this is a linearly independent set of eigenvectors, so our lemma guarantees the existence of a matrix <m>P</m>
     such that
     <me>
       P^{-1}AP = \bbm \lambda I_k \amp B\\0\amp A_1\ebm
     </me>.
     Let <m>\tilde{A}=P^{-1}AP</m>. On the one hand, since <m>\tilde{A}\sim A</m>,
     we have <m>c_A(x)=c_{\tilde{A}}(x)</m>.
     On the other hand,
     <me>
       \det(xI_n-\tilde{A}) = \det\bbm (x-\lambda)I_k \amp -B\\0 \amp xI_{n-k}-A_1\ebm = (x-\lambda)^k\det(xI_{n-k}-A_1)
     </me>.
     This shows that <m>c_A(x)</m> is divisible by <m>(x-\lambda)^k</m>.
     Since <m>m</m> is the largest integer such that <m>c_A(x)</m> is divisible by <m>(x-\lambda)^m</m>,
     we must have <m>\dim E_\lambda(A)=k\leq m</m>.
  </p>

  <p>
    Another important result is the following.
    The proof is a bit tricky: it requires mathematical induction,
    and a couple of clever observations.
  </p>

  <theorem xml:id="thm-eigen-independent">
    <statement>
      <p>
        Let <m>\vv_1,\ldots, \vv_k</m> be eigenvectors corresponding to distinct
        eigenvalues <m>\lambda_1,\ldots, \lambda_k</m> of a matrix <m>A</m>.
        Then <m>\{\vv_1,\ldots, \vv_k\}</m> is linearly independent.
      </p>
    </statement>
    <proof>
      <p>
        The proof is by induction on the number <m>k</m> of distinct eigenvalues.
        Since eigenvectors are nonzero, any set consisting of a single eigenvector <m>\vv_1</m> is independent.
        Suppose, then, that a set of eigenvectors corresponding to <m>k-1</m> distinct eigenvalues is independent,
        and let <m>\vv_1,\ldots, \vv_k</m> be eigenvectors corresponding to distinct eigenvalues <m>\lambda_1,\ldots, \lambda_k</m>.
      </p>

      <p>
        Consider the equation
        <me>
          c_1\vv_1+c_2\vv_2+\cdots +c_k\vv_k=\zer
        </me>,
        for scalars <m>c_1,\ldots, c_k</m>. Multiplying both sides by the matrix <m>A</m>, we have
        <mdn>
          <mrow>\zer \amp = A\zer</mrow>
          <mrow> A(c_1\vv_1+c_2\vv_2+\cdots +c_k\vv_k)</mrow>
          <mrow> \amp =c_1A\vv_1+c_2A\vv_2+\cdots + c_kA\vv_k</mrow>
          <mrow xml:id="eqn-eigen-indep1"> \amp =c_1\lambda_1\vv_1+c_2\lambda_2\vv_2+\cdots + c_k\lambda_k\vv_k</mrow>
        </mdn>.
      </p>

      <p>
        On the other hand, we can also multiply both sides by the eigenvalue <m>\lambda_1</m>, giving
        <men xml:id="eqn-eigen-indep2">
          \zer = c_1\lambda_1\vv_1 + c_2\lambda_1\vv_2+\cdots + c_k\lambda_1\vv_k
        </men>.
        Subtracting <xref ref="eqn-eigen-indep2"/> from <xref ref="eqn-eigen-indep1"/>,
        the first temrs cancel, and we get
        <me>
          c_2(\lambda_2-\lambda_1)\vv_2+\cdots + c_k(\lambda_k-\lambda_1)\vv_k=\zer
        </me>.
      </p>

      <p>
        By hypothesis, the set <m>\{\vv_2,\ldots, \vv_k\}</m> of <m>k-1</m> eigenvectors is linearly independent.
        We know that <m>\lambda_j-\lambda_1\neq 0</m> for <m>j=2,\ldots, k</m>,
        since the eigenvalues are all distinct.
        Therefore, the only way this linear combination can equal zero is if <m>c_2=0,\ldots, c_k=0</m>.
        This leaves us with <m>c_1\vv_1=\zer</m>, but <m>\zz_1\neq \zer</m>, so <m>c_1=0</m> as well.
      </p>
    </proof>

  </theorem>


  <p>
    <xref ref="thm-eigen-independent"/> tells us that vectors from different eigenspaces are independent.
    In particular, a union of bases from each eigenspace will be an independent set.
    Therefore, <xref ref="thm-multiplicity"/> provides an initial criterion for diagonalization:
    if the dimension of each eigenspace <m>E_\lambda(A)</m> is equal to the multiplicity of <m>\lambda</m>,
    then <m>A</m> is diagonalizable.
  </p>

  <p>
    Our focus in the next section will be on diagonalization of <em>symmetric</em>
    matrices, and soon we will see that for such matrices, eigenvectors corresponding to different eigenvalues are
    not just independent, but <em>orthogonal</em>.
  </p>
</section>
