<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="section-projection">
  <title>Orthogonal Projection</title>
  <p>
    In <xref ref="ex-test-span">Exercise</xref>, we saw that the <xref ref="thm-fourier-expansion" text="title"/>
    gives us an efficient way of testing whether or not a vector belongs to the span of an orthogonal set.
    When the answer is <q>no</q>, the quantity we compute while testing turns out to be very useful:
    it gives the <em>orthogonal projection</em> of that vector onto the span of our orthogonal set.
    This turns out to be exactly the ingredient needed to solve certain minimum distance problems.
  </p>

  <p>
    We hinted above that the calculations we've been doing have a lot to do with projection.
    Since any single nonzero vector forms an orthogonal basis for its span,
    the projection
    <me>
      \proj{\uu}{\vv}=\left(\frac{\uu\dotp\vv}{\len{\uu}^2}\right)\uu
    </me>
    can be viewed as the orthogonal projection of the vector <m>\vv</m>,
    not onto the vector <m>\uu</m>, but onto the subspace <m>\spn\{\uu\}</m>.
    This is, after all, how we viewed projections in elementary linear algebra:
    we drop the perpendicular from the tip of <m>\vv</m> onto the <em>line</em> in the direction of <m>\uu</m>.
  </p>

  <p>
    Now that we know how to define an orthogonal basis for a subspace,
    we can define orthogonal projection onto subspaces of dimension greater than one.
  </p>

  <definition xml:id="def-ortho-projection">
    <statement>
      <p>
        Let <m>U</m> be a subspace of <m>\R^n</m> with orthogonal basis
        <m>\{\uu_1,\ldots, \uu_k\}</m>.
        For any vector <m>\vv\in \R^n</m>, we define the <term>orthogonal projection</term>
        of <m>\vv</m> onto <m>U</m> by
        <me>
          \proj{U}{\vv} = \sum_{i=1}^k\left(\frac{\uu_i\dotp\vv}{\len{\uu_i}^2}\right)\uu_i
        </me>.
      </p>
    </statement>
  </definition>

  <p>
    Note that <m>\proj{U}{\vv}</m> is indeed an element of <m>U</m>, since it's a linear combination of its basis vectors.
    In the case of the trivial subspace <m>U=\{\zer\}</m>, we define orthogonal projection of any vector to be <m>\zer</m>,
    since really, what other choice do we have? (This case isn't really of any interest, we just like being thorough.)
  </p>

  <p>
    Let's see how this might be put to use in a classic problem: finding the distance from a point to a plane.
  </p>

  <aside vshift="0">
    <p>
      One limitation of this approach to projection is that we must project onto a <em>subspace</em>.
      Given a plane like <m>x-2y+4z=4</m>, we would need to modify our approach.
      One way to do it would be to find a point on the plane,
      and then try to translate everything to the origin.
      It's interesting to think about how this might be accomplished
      (in particular, in what direction would the translation have to be performed?)
      but somewhat external to the questions we're interested in here.
    </p>
  </aside>

  <example xml:id="eg-proj-distance">
    <statement>
      <p>
        Find the distance from the point <m>(3,1,-2)</m> to the plane <m>P</m>
        defined by <m>x-2y+4z=0</m>.
      </p>
    </statement>
    <solution>
      <title>Using projection onto a normal vector</title>

      <p>
        In an elementary linear algebra (or calculus) course, we would solve this problem as follows.
        First, we would need two vectors parallel to the plane.
        If <m>\bbm x\\y\\z\ebm</m> lies in the plane, then <m>x-2y+4z=0</m>, so <m>x=2y-4z</m>, and
        <me>
          \bbm x\\y\\z\ebm = \bbm 2y-4z\\y\\z\ebm = y\bbm 2\\1\\0\ebm + z\bbm -4\\0\\1\ebm
        </me>,
        so <m>\uu=\bbm 2\\1\\0\ebm</m> and <m>\vv\bbm -4\\0\\1\ebm</m> are parallel to the plane.
        We then compute the normal vector
        <me>
          \mathbf{n}=\uu\times\vv=\bbm 1\\-2\\4\ebm
        </me>,
        and compute the projection of the position vector <m>\mathbf{p}=\bbm 3,1,-2\ebm</m> for the point <m>P=(3,1,-2)</m> onto <m>\mathbf{n}</m>.
        This gives the vector
        <me>
          \xx = \left(\frac{\mathbf{p}\dotp\mathbf{n}}{\len{\mathbf{n}}^2}\right)\mathbf{n} = \frac{-7}{21}\bbm 1\\-2\\4\ebm =\bbm-1/3\\2/3\\-4/3\ebm
        </me>.
      </p>

      <p>
        Now, this vector is <em>parallel</em> to <m>\mathbf{n}</m>, so it's perpendicular to the plane.
        Subtracting it from <m>\mathbf{p}</m> gives a vector parallel to the plane, and this is the position vector for the point we seek.
        <me>
          \mathbf{q}=\mathbf{p}-\xx=\bbm 3\\1\\-2\ebm-\bbm -1/3\\-2/3\\-4/3\ebm = \bbm 10/3\\1/3\\-2/3\ebm
        </me>
        so the closest point is <m>Q=\bigl(\frac{10}{3},\frac13,-\frac{2}{3}\bigr)</m>.
        We weren't asked for it, but note that if we wanted the distance from the point <m>P</m> to the plane,
        this is given by <m>\len{\xx}=\frac13\sqrt{21}</m>.
      </p>
    </solution>
    <solution>
      <title>Using orthogonal projection</title>

      <p>
        Let's solve the same problem using orthogonal projection.
        First, we have to deal with the fact that the vectors <m>\uu</m> and <m>\vv</m> are probably not orthogonal.
        To get around this, we replace <m>\vv</m> with
        <me>
          \ww = \vv-\left(\frac{\vv\dotp\uu}{\len{\uu}^2}\right)\uu = \bbm -4\\0\\1\ebm+\frac 85\bbm 2\\1\\0\ebm = \bbm -4/5\\8/5\\1\ebm
        </me>.
        We now set
        <md>
          <mrow>\mathbf{q} \amp =\left(\frac{\mathbf{p}\dotp\uu}{\len{\uu}^2}\right)\uu-\left(\frac{\mathbf{p}\dotp\ww}{\len{\ww}^2}\right)\ww</mrow>
          <mrow> \amp = \frac{7}{5}\bbm 2\\1\\0\ebm +\frac{-14}{105}\bbm -4\\8\\5\ebm </mrow>
          <mrow> \amp = \bbm 10/3\\1/3\\-2/3\ebm</mrow>
        </md>.
        Lo and behold, we get the same answer as before.
      </p>
    </solution>
  </example>

  <p>
    The only problem with <xref ref="def-ortho-projection"/> is that it appears to depend on the choice of orthogonal basis.
    To see that it doesn't, we need one more definition.
  </p>

  <definition xml:id="def-ortho-comp">
    <statement>
      <p>
        For any subspace <m>U</m> of <m>\R^n</m>,
        we define the <term>orthogonal complement</term> of <m>U</m>, denoted <m>U^\bot</m>,
        by
        <me>
          U^\bot = \{\xx\in\R^n \,|\, \xx\dotp\yy = 0 \text{ for all } \yy\in U\}
        </me>.
      </p>
    </statement>
  </definition>

  <p>
    The term <q>complement</q> comes from terminology we mentioned early on,
    but didn't spend much time on. <xref ref="thm-construct-complement">Theorem</xref>
    told us that for any subspace <m>U</m> of a vector space <m>V</m>,
    it is possible to construct another subspace <m>W</m> of <m>V</m>
    such that <m>V = U\oplus W</m>.
    The subspace <m>W</m> is known as a complement of <m>U</m>.
    A complement is not unique, but the orthogonal complement is.
    As you might guess from the name, <m>U^\bot</m> is also a subspace of <m>\R^n</m>.
  </p>

  <exercise xml:id="ex-comp-subspace" label="ex-comp-subspace">
    <statement>
      <p>
        Show that <m>U^\bot</m> is a subspace of <m>\R^n</m>.
      </p>
    </statement>
    <response/>
    <hint>
      <p>
        The trusty <xref ref="thm-subspace-test" text="title"/> is your friend here.
        Just be careful to work correctly with the definition of <m>U^\bot</m>.
      </p>
    </hint>
  </exercise>

  <theorem xml:id="thm-projection">
    <title>Projection Theorem</title>
    <statement>
      <p>
        Let <m>U</m> be a subspace of <m>\R^n</m>, let <m>\xx</m> be any vector in <m>\R^n</m>,
        and let <m>\mathbf{p}=\proj{U}{\xx}</m>. Then:
        <ol>
          <li>
            <p>
              <m>\mathbf{p}\in U</m>, and <m>\xx-\mathbf{p}\in U^\bot</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\mathbf{p}</m> is the <em>closest</em> vector in <m>U</m> to the vector <m>\xx</m>,
              in the sense that the distance <m>d(\mathbf{p},\xx)</m> is minimal among all vectors in <m>U</m>.
              That is, for all <m>\uu\neq \mathbf{p}\in U</m>, we have
              <me>
                \len{\xx-\mathbf{p}}\lt\len{\xx-\uu}
              </me>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <title>Strategy</title>
      <p>
        For the first part, review the <xref ref="thm-orthogonal-lemma" text="title"/>,
        and convince yourself that this says the same thing.
        The second part is the hard part, and it requires a trick:
        we can write <m>\xx-\uu</m> as <m>(\xx-\mathbf{p})+(\mathbf{p}-\uu)</m>,
        and then notice that <m>\mathbf{p}-\uu\in U</m>.
        What can we say using the first part, and the Pythagorean theorem?
      </p>
    </proof>
    <proof>
      <p>
        By <xref ref="def-ortho-projection"/>, <m>\mathbf{p}</m> is a linear combination
        of elements in <m>U</m>, so <m>\mathbf{p}\in U</m>.
        The fact that <m>\xx-\mathbf{p}\in U^\bot</m> follows directly from the <xref ref="thm-orthogonal-lemma" text="title"/>.
      </p>

      <p>
        Choose any <m>\uu\in U</m> with <m>\uu \neq \mathbf{p}</m>, and write
        <me>
          \xx-\uu = (\xx-\mathbf{p})+(\mathbf{p}-\uu)
        </me>.
        Since <m>\mathbf{p}-\uu\in U</m> and <m>\xx-\mathbf{p}\in U^\bot</m>,
        we know that these two vectors are orthogonal, and therefore,
        <me>
          \len{\xx-\uu}^2=\len{\xx-\mathbf{p}}^2+\len{\mathbf{p}-\uu}^2\gt \len{\xx-\mathbf{p}}^2
        </me>,
        by the <xref ref="thm-pythagoras" text="title"/>.
      </p>
    </proof>
  </theorem>

  <exercise xml:id="ex-ortho-comp" label="ex-ortho-comp">
    <statement>
      <p>
        Show that <m>U\cap U^\bot = \{\zer\}</m>.
        Use this fact to show that <xref ref="def-ortho-projection"/> does not depend on the choice orthogonal basis.
      </p>
    </statement>
    <response/>
    <hint>
      <p>
        Suppose we find vectors <m>\mathbf{p}</m> and <m>\mathbf{p}'</m> using basis <m>B</m> and <m>B'</m>.
        Note that <m>\mathbf{p}-\mathbf{p}'\in U</m>, but also that
        <me>
          \mathbf{p}-\mathbf{p}' = (\mathbf{p}-\xx)-(\mathbf{p}'-\xx)
        </me>
        Now use <xref ref="thm-projection"/>.
      </p>
    </hint>
  </exercise>


  <p>
    Finally, we note one more useful fact. The process of sending a vector to its orthogonal projection defines an operator on <m>\R^n</m>,
    and yes, it's linear.
  </p>

  <theorem xml:id="thm-proj-operator">
    <statement>
      <p>
        Let <m>U</m> be a subspace of <m>\R^n</m>, and define a function <m>P_U:\R^n\to \R^n</m> by
        <me>
          P_U(\xx) = \proj{U}{\xx} \text{ for any } \xx\in\R^n
        </me>.
        Then <m>P_U</m> is a linear operator such that <m>U=\im P_U</m> and <m>U^\bot = \ker P_U</m>.
      </p>
    </statement>
    <proof>
      <title>Strategy</title>
      <p>
        The fact that <m>P_U</m> is linear follows from properties of the dot product, and some careful checking.
        We know that <m>\im P_U\subseteq U</m> by definition of the projection,
        and you can show that <m>P_U</m> acts as the identity on <m>U</m> using the <xref ref="thm-fourier-expansion" text="title"/>.
      </p>

      <p>
        If <m>\xx\in U^\bot</m>, then <m>P_U(\xx)=\zer</m> by definition of <m>P_U</m>.
        (Recall that it is defined using dot products with vectors in <m>U</m>.)
        If <m>\xx\in \ker P_U</m>, use the <xref ref="thm-projection" text="title"/>,
        to show that <m>\xx\in U^\bot</m>.
      </p>
    </proof>
  </theorem>

  <remark xml:id="rem-orth-comp-dim">
    <p>
      It follows from this result and the <xref ref="thm-dimension-lintrans" text="title"/> that
      <me>
        \dim U + \dim U^\bot = n
      </me>,
      and since <m>U\cap U^\bot = \{\zer\}</m>, <m>U^\bot</m> is indeed a complement of <m>U</m>
      in the sense introduced in <xref ref="thm-construct-complement">Theorem</xref>.
      It's also fairly easy to see that <m>\dim U + \dim U^\bot = n</m> directly.
      If <m>\ww\in U^\bot</m>, and <m>\{\uu_1,\ldots, \uu_k\}</m>
      is a basis for <m>U</m>, then we have
      <me>
        \ww\dotp \uu_1= 0, \ldots, \ww\dotp \uu_k=0
      </me>,
      and for an unknown <m>\ww</m>, this is simply a homogeneous system of <m>k</m> equations with <m>n</m> variables.
      Moreover, they are <em>independent</em> equations, since the <m>\uu_i</m> form a basis.
      We thus expect <m>n-k</m> free parameters in the general solution.
    </p>
  </remark>

  <theorem xml:id="thm-ortho-comp">
    <statement>
      <p>
        For any subspace <m>U</m> of <m>\R^n</m>, we have
        <me>
          U\oplus U^\bot = \R^n
        </me>.
      </p>
    </statement>
  </theorem>

  <aside vshift="0">
    <p>
      Note that if <m>U=\{\zer\}</m>, then <m>U^\bot=\R^n</m>,
      and if <m>U=\R^n</m>, then <m>U^\bot = \{\zer\}</m>.
      (Can you prove this?)
    </p>
  </aside>

  <exercise xml:id="ex-tf-orth-comp" label="ex-tf-orth-comp">
    <statement correct="no">
      <p>
        Given subspaces <m>U,W</m> of <m>\R^n</m> with <m>U\cap W=\{\zer\}</m>,
        if <m>U\oplus W=\R^n</m>, then <m>W=U^\bot</m>.
      </p>
    </statement>
    <feedback>
      <p>
        A subspace can have many complements, but only one orthgonal complement.
        For example, a complement to the <m>x</m> axis in <m>\R^2</m> is given by
        any other line through the origin, but only the <m>y</m> axis is orthogonal.
      </p>
    </feedback>
  </exercise>

  <theorem xml:id="thm-complement-matrix">
    <statement>
      <p>
        Let <m>U</m> be a subspace of <m>\R^n</m>, with basis <m>\{\uu_1,\ldots, \uu_k\}</m>.
        Let <m>A</m> be the <m>n\times k</m> matrix whose columns are the basis vectors for <m>U</m>.
        Then <m>U^\bot = \nll(A^T)</m>.
      </p>
    </statement>
  </theorem>

  <p>
    <xref ref="thm-complement-matrix"/> tells us that we can find a basis for <m>U^\bot</m>
    by solving the homogeneous system <m>A^T\xx=\zer</m>.
    Make sure you can see why this is true!
  </p>


  <example xml:id="eg-comp-basis">
    <statement>
      <p>
        Let <m>U = \{(a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c)\,|\, a,b,c\in\R\}\subseteq \R^5</m>.
        Determine a basis for <m>U^\bot</m>.
      </p>
    </statement>
    <response/>
    <solution>
      <p>
        First, we note that for a general element of <m>U</m>, we have
        <me>
          (a-b+3c, 2a+b, 3c, 4a-b+3c,a-4c) = a(1,2,0,4,1)+b(-1,1,0,-1,0)+c(3,0,3,3,-4)
        </me>,
        so <m>\{(1,2,0,4,1),(-1,1,0,-1,0),(3,0,3,3,-4)\}</m> is a basis for <m>U</m>.
        (We have just shown that this set spans <m>U</m>;
        it is independent since the first two vectors are not parallel,
        and the third vector cannot be in the span of the first two, since its third entry is nonzero.)
        As in <xref ref="thm-complement-matrix"/>,
        we set <m>A=\bbm 1\amp -1\amp 3\\2\amp 1\amp 0\\0\amp 0\amp 3\\4\amp -1\amp 3\\1\amp 0\amp -4\ebm</m>.
      </p>

      <p>
        To find a basis for <m>U^\bot</m>, we simply need to find the nullspace of <m>A^T</m>,
        which we do below.
      </p>

      <sage>
        <input>
          from sympy import Matrix, init_printing
          init_printing()
          A = Matrix(5,3,[1,-1,3,2,1,0,0,0,3,4,-1,3,1,0,-4])
          B=A.T
          B.nullspace()
        </input>
        <output>
          \[\left[\bbm -2\\-1\\1\\1\\0\ebm, \bbm -\frac13\\ -\frac13\\ \frac53\\0\\1\ebm\right]\]
        </output>
      </sage>
    </solution>
  </example>

  <exercises>
    <exercise xml:id="ex-proj-op-id" label="ex-proj-op-id">
      <statement>
        <p>
          Prove that for any subspace <m>U\subseteq \R^n</m>,
          <m>\proj{U}{} + \proj{U^\bot}{}</m> is the identity operator on <m>\R^n</m>.
        </p>
      </statement>
      <response/>
      <hint>
        <p>
          Given <m>\xx\in\R^n</m>, can you write it as a sum of an element of <m>U</m>
          and an element of <m>U^\bot</m>?
        </p>
      </hint>
    </exercise>

    <exercise xml:id="ex-comp-comp" label="ex-comp-comp">
      <statement>
        <p>
          Prove that for any subspace <m>U\subseteq \R^n</m>, <m>(U^\bot)^\bot = U</m>.
        </p>
      </statement>
      <response/>
      <hint>
        <p>
          Show that <m>U\subseteq (U^\bot)^\bot</m>, and then use <xref ref="rem-orth-comp-dim"/>
          to show that the two spaces must have the same dimension.
        </p>
      </hint>
    </exercise>

    <exercise xml:id="ex-comp-sum" label="ex-comp-sum">
      <statement>
        <p>
          Let <m>U</m> and <m>W</m> be subspaces of <m>\R^n</m>.
          Prove that <m>(U+W)^\bot = U^\bot\cap W^\bot</m>.
        </p>
      </statement>
      <response/>
      <hint>
        <p>
          One inclusion is easier than the other.
          Use <xref ref="thm-sum-dimension"/> and <xref ref="rem-orth-comp-dim"/>
          to show that the dimensions must be equal.
        </p>
      </hint>
    </exercise>
    
    <exercise xml:id="ex-ww-proj1" label="ex-ww-proj1">
      <webwork source="local/Library/TCNJ/TCNJ_OrthogonalSets/problem10.pg" />
    </exercise>
    
    <exercise xml:id="ex-ww-proj2" label="ex-ww-proj2">
      <webwork source="local/Library/TCNJ/TCNJ_LengthOrthogonality/problem3.pg" />
    </exercise>

    <exercise xml:id="ex-ww-proj3" label="ex-ww-proj3">
      <webwork source="local/Library/Rochester/setLinearAlgebra17DotProductRn/ur_la_17_20.pg" />
    </exercise>

    <exercise xml:id="ex-ww-proj4" label="ex-ww-proj4">
      <webwork source="local/Library/TCNJ/TCNJ_OrthogonalProjections/problem2.pg" />
    </exercise>

    <exercise xml:id="ex-ww-proj5" label="ex-ww-proj5">
      <webwork source="local/Library/TCNJ/TCNJ_OrthogonalProjections/problem1.pg" />
    </exercise>

    <exercise xml:id="ex-ww-proj6" label="ex-ww-proj6">
      <webwork source="local/Library/Rochester/setLinearAlgebra17DotProductRn/ur_la_17_15.pg" />
    </exercise>
  </exercises>
</section>
