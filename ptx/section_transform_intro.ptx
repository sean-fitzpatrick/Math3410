<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="sec-lin-tran-intro">
  <title>Definition and examples</title>

  <p>
    Let <m>V</m> and <m>W</m> be vector spaces. At their most basic,
    all vector spaces are sets. Given any two sets, we can consider functions from one to the other.
    The functions of interest in linear algebra are those that respect the vector space structure of the sets.
  </p>

  <definition xml:id="def-lin-trans">
    <statement>
      <p>
        Let <m>V</m> and <m>W</m> be vector spaces.
        A function <m>T:V\to W</m> is called a <term>linear transformation</term> if:
        <ol>
          <li>
            <p>
              For all <m>\vv_1,\vv_2\in V</m>, <m>T(\vv_1+\vv_2)=T(\vv_1)+T(\vv_2)</m>.
            </p>
          </li>
          <li>
            <p>
              For all <m>\vv\in V</m> and scalars <m>c</m>, <m>T(c\vv)=cT(\vv)</m>.
            </p>
          </li>
        </ol>
        We often use the term <term>linear operator</term>
        to refer to a linear transformation <m>T:V\to V</m> from a vector space to itself.
      </p>
    </statement>
  </definition>

  <p>
    Note on notation: it is common usage to drop the usual parentheses of function notation
    when working with linear transformations, as long as this does not cause confusion.
    That is, one might write <m>T\vv</m> instead of <m>T(\vv)</m>,
    but one should never write <m>T\vv+\ww</m> in place of <m>T(\vv+\ww)</m>,
    for the same reason that one should never write <m>2x+y</m> in place of <m>2(x+y)</m>.
    Mathematicians often think of linear transformations in terms of matrix multiplication,
    which probably explains this notation to some extent.
  </p>

  <p>
    The properties of a linear transformation tell us that a linear map <m>T</m>
    <em>preserves</em> the operations of addition and scalar multiplication.
    (When the domain and codomain are different vector spaces, we might say that <m>T</m> <em>intertwines</em>
    the operations of the two vector spaces.)
    In particular, any linear transformation <m>T</m> must preserve the zero vector,
    and respect linear combinations.
  </p>

  <theorem xml:id="thm-lt-props">
    <statement>
      <p>
        Let <m>T:V\to W</m> be a linear transformation. Then
        <ol>
          <li>
            <p>
              <m>T(\mathbf{0}_V) = \mathbf{0}_W</m>, and
            </p>
          </li>
          <li>
            <p>
              For any scalars <m>c_1,\ldots, c_n</m> and vectors <m>\vv_1,\ldots, \vv_n\in V</m>,
              <me>
                T(c_1\vv_1+c_2\vv_2+\cdots + c_n\vv_n) = c_1T(\vv_1)+c_2T(\vv_2)+\cdots + c_nT(\vv_n)
              </me>.
            </p>
          </li>
        </ol>
      </p>
    </statement>
    <proof>
      <p>
        <ol>
          <li>
            <p>
              Since <m>\mathbf{0}_V+\mathbf{0}_V = \mathbf{0}_V</m>, we have
              <me>
                T(\mathbf{0}_V) = T(\mathbf{0}_V+\mathbf{0}_V) = T(\mathbf{0}_V)+T(\mathbf{0}_V)
              </me>.
              Adding <m>-T(\mathbf{0}_V)</m> to both sides of the above gives us <m>\mathbf{0}_W = T(\mathbf{0}_V)</m>.
            </p>
          </li>
          <li>
            <p>
              The addition property of a linear transformation can be extended to sums of three or more vectors using associativity.
              Therefore, we have
              <md>
                <mrow>T(c_1\vv_1+\cdots + c_n\vv_n) \amp = T(c_1\vv_1)+ \cdots T(c_n\vv_n)</mrow>
                <mrow> \amp = c_1T(\vv_1)+\cdots +c_nT(\vv_n)</mrow>
              </md>,
              where the second line follows from the scalar multiplication property.
            </p>
          </li>
        </ol>
      </p>
    </proof>

  </theorem>


  <example xml:id="ex-matrix-trans">
    <statement>
      <p>
        Let <m>V=\R^n</m> and let <m>W=\R^m</m>.
        For any <m>m\times n</m> matrix <m>A</m>, the map <m>T_A:\R^n\to \R^m</m> defined by
        <me>
          T_A(\xx) = A\xx
        </me>
        is a linear transformation. (This follows immediately from properties of matrix multiplication.)
      </p>

      <p>
        Let <m>B = \{\mathbf{e}_1,\ldots, \mathbf{e}_n\}</m> denote the standard basis of <m>\R^n</m>.
        Recall that <m>A\mathbf{e}_i</m> is equal to the <m>i</m>th column of <m>A</m>.
        Thus, if we know the value of a linear transformation <m>T:\R^n\to \R^m</m> on each basis vector,
        we can immediately determine the matrix <m>A</m> such that <m>T=T_A</m>:
        <me>
          A = \bbm T(\mathbf{e}_1) \amp T(\mathbf{e}_2) \amp \cdots \amp T(\mathbf{e}_n)\ebm
        </me>.
        This is true because <m>T</m> and <m>T_A</m> agree on the standard basis: for each <m>i=1,2,\ldots, n</m>,
        <me>
          T_A(\mathbf{e}_i) = A\mathbf{e}_i = T(\mathbf{e}_i)
        </me>.
        Moreover, if two linear transformations agree on a basis, they must be equal.
        Given any <m>\xx\in \R^n</m>, we can write <m>\xx</m> uniquely as a linear combination
        <me>
          \xx=c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots + c_n\mathbf{e}_n.
        </me>
        If <m>T(\mathbf{e}_i)=T_A(\mathbf{e}_i)</m> for each <m>i</m>, then by <xref ref="thm-lt-props"/> we have
        <md>
          <mrow>T(\xx) \amp = T(c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots + c_n\mathbf{e}_n) </mrow>
          <mrow> \amp = c_1T(\mathbf{e}_1)+c_2T(\mathbf{e}_2)+\cdots + c_nT(\mathbf{e}_n)</mrow>
          <mrow> \amp = c_1T_A(\mathbf{e}_1)+c_2T_A(\mathbf{e}_2)+\cdots + c_nT_A(\mathbf{e}_n)</mrow>
          <mrow> \amp = T_A(c_1\mathbf{e}_1+c_2\mathbf{e}_2+\cdots + c_n\mathbf{e}_n) </mrow>
          <mrow> \amp = T_A(\xx)</mrow>
        </md>.
      </p>
    </statement>
  </example>

  <p>
    Let's look at some other examples of linear transformations.
    <ul>
      <li>
        <p>
          For any vector spaces <m>V,W</m> we can define the <term>zero transformation</term> <m>0:V\to W</m>
          by <m>0(\vv)=\mathbf{0}</m> for all <m>\vv\in V</m>.
        </p>
      </li>
      <li>
        <p>
          On any vector space <m>V</m> we have the <term>identity transformation</term>
          <m>1_V:V\to V</m> defined by <m>1_V(\vv)=\vv</m> for all <m>\vv\in V</m>.
        </p>
      </li>
      <li>
        <p>
          Let <m>V = F[a,b]</m> be the space of all functions <m>f:[a,b]\to \R</m>.
          For any <m>c\in [a,b]</m> we have the <term>evaluation map</term>
          <m>E_a: V\to \R</m> defined by <m>E_a(f) = f(a)</m>.
        </p>

        <p>
          To see that this is linear, note that <m>E_a(0)=\mathbf{0}(a)=0</m>,
          where <m>\mathbf{0}</m> denotes the zero function;
          for any <m>f,g\in V</m>,
          <me>
            E_a(f+g)=(f+g)(a)=f(a)+g(a)=E_a(f)+E_a(g)
          </me>,
           and for any scalar <m>c\in \R</m>,
           <me>
             E_a(cf) = (cf)(a) = c(f(a))=cE_a(f)
           </me>.
        </p>

        <p>
          Note that the evaluation map can similarly be defined as a linear transformation on any vector space of polynomials.
        </p>
      </li>

      <li>
        <p>
          On the vector space <m>C[a,b]</m> of all <em>continuous</em> functions on <m>[a,b]</m>,
          we have the integration map <m>I:C[a,b]\to \R</m> defined by
          <m>I(f)=\int_a^b f(x)\,dx</m>. The fact that this is a linear map follows from properties of integrals proved in a calculus class.
        </p>
      </li>

      <li>
        <p>
          On the vector space <m>C^1(a,b)</m> of continuously differentiable functions on <m>(a,b)</m>,
          we have the differentiation map <m>D: C^1(a,b)\to C(a,b)</m> defined by
          <m>D(f) = f'</m>. Again, linearity follows from properties of the derivative.
        </p>
      </li>

      <li>
        <p>
          Let <m>\R^\infty</m> denote the set of sequences <m>(a_1,a_2,a_3,\ldots)</m> of real numbers,
          with term-by-term addition and scalar multiplication.
          The shift operators
          <md>
            <mrow>S_L(a_1,a_2,a_3,\ldots)  \amp = (a_2,a_3,a_4,\ldots) </mrow>
            <mrow>S_R(a_1,a_2,a_3,\ldots) \amp = (0,a_1,a_2,\ldots)</mrow>
          </md>
          are both linear.
        </p>
      </li>

      <li>
        <p>
          On the space <m>M_{mn}(\R)</m> of <m>m\times n</m> matrices,
          the trace defines a linear map <m>\operatorname{tr}:M_{mn}(\R)\to \R</m>,
          and the transpose defines a linear map <m>T:M_{mn}(\R)\to M_{nm}(\R)</m>.
          The determinant and inverse operations on <m>M_{nn}</m> are <em>not</em> linear.
        </p>
      </li>
    </ul>
  </p>

  <p>
    For finite-dimensional vector spaces, it is often convenient to work in terms of a basis.
    The properties of a linear transformation tell us that we can completely define any linear transformation by giving its values on a basis.
    In fact, it's enough to know the value of a transformation on a spanning set.
    The argument given in <xref ref="ex-matrix-trans"/> can be applied to any linear transformation, to obtain the following result.
  </p>

  <theorem xml:id="thm-agree-span">
    <statement>
      <p>
        Let <m>T:V\to W</m> and <m>S:V\to W</m> be two linear transformations.
        If <m>V = \spn\{\vv_1,\ldots, \vv_n\}</m> and <m>T(\vv_i)=S(\vv_i)</m>
        for each <m>i=1,2,\ldots, n</m>, then <m>T=S</m>.
      </p>
    </statement>
  </theorem>

  <p>
    <alert>Caution</alert>: If the above spanning set is not also independent,
    then we can't just define the values <m>T(\vv_i)</m> however we want.
    For example, suppose we want to define <m>T:\R^2\to\R^2</m>,
    and we set <m>\R^2=\spn{(1,2),(4,-1),(5,1)}</m>.
    If <m>T(1,2)=(3,4)</m> and <m>T(4,-1)=(-2,2)</m>,
    then we <em>must</em> have <m>T(5,1)=(1,6)</m>.
    Why? Because <m>(5,1)=(1,2)+(4,1)</m>, and if <m>T</m> is to be linear,
    then we have to have <m>T((1,2)+(4,-1))=T(1,2)+T(4,-1)</m>.
  </p>

  <p>
    If for some reason we already know that our transformation is linear,
    we might still be concerned about the fact that if a spanning set is not independent,
    there will be more than one way to express a vector as linear combination of vectors in that set.
    If we define <m>T</m> by giving its values on a spanning set, will it be well-defined?
    (That is, could we get two different values for <m>T(\vv)</m> by expressing <m>\vv</m> in terms of the spanning set in two different ways?)
    Suppose that we have scalars <m>a_1,\ldots, a_n, b_1,\ldots, b_n</m> such that
    <md>
      <mrow>\vv \amp = a_1\vv_1+\cdots + a_n\vv_n \quad \text{ and }</mrow>
      <mrow>\vv \amp = b_1\vv_1+\cdots + b_n\vv_n</mrow>
    </md>
    We then have
    <md>
      <mrow>a_1T(\vv_1)+\cdots + a_nT(\vv_n) \amp =T(a_1\vv_1+\cdots + a_n\vv_n) </mrow>
      <mrow> \amp =T(b_1\vv_1+\cdots +b_n\vv_n)</mrow>
      <mrow>  \amp =b_1T(\vv_1)+\cdots +b_nT(\vv_n)</mrow>
    </md>.
  </p>

  <p>
    Of course, we can avoid all of this unpleasantness by using a <em>basis</em>
    to define a transformation. Given a basis <m>B = \{\vv_1,\ldots, \vv_n\}</m>
    for a vector space <m>V</m>, we can define a transformation <m>T:V\to W</m>
    by setting <m>T(\vv_i)=\ww_i</m> for some choice of vectors <m>\ww_1,\ldots, \ww_n</m>
    and defining
    <me>
      T(c_1\vv_1+\cdots +c_n\vv_n)=c_1T(\vv_1)+\cdots + c_nT(\vv_n)
    </me>.
    Because each vector <m>\vv\in V</m> can be written <em>uniquely</em> in terms of a basis,
    we know that our transformation is well-defined.
  </p>

  <p>
    The next theorem seems like an obvious consequence of the above,
    and indeed, one might wonder where the assumption of a basis is needed.
    The distinction here is that the vectors <m>\ww_1,\ldots, \ww_n\in W</m>
    are chosen in advance, and then we set <m>T(vec{b}_i)=\ww_i</m>,
    rather than simply defining each <m>\ww_i</m> as <m>T(\mathbf{b}_i)</m>.
  </p>

  <theorem xml:id="thm-define-using-basis">
    <statement>
      <p>
        Let <m>V,W</m> be vector spaces. Let <m>B=\{\mathbf{b}_1,\ldots, \mathbf{b}_n\}</m>
        be a basis of <m>V</m>, and let <m>\ww_1,\ldots, \ww_n</m> be any vectors in <m>W</m>.
        (These vectors need not be distinct.)
        Then there exists a unique linear transformation <m>T:V\to W</m> such that
        <m>T(\mathbf{b}_i)=\ww_i</m> for each <m>i=1,2,\ldots, n</m>; indeed,
        we can define <m>T</m> as follows:
        given <m>\vv\in V</m>, write <m>\vv=c_1\vv_1+\cdots +c_n\vv_n</m>. Then
        <me>
          T(\vv)=T(c_1\vv_1+\cdots + c_n\vv_n) = c_1\ww_1+\cdots +c_n\ww_n
        </me>.
      </p>
    </statement>
  </theorem>

  <p>
    With the basic theory out of the way, let's look at a few basic examples.
  </p>

  <exercise>
    <statement>
      <p>
        Suppose <m>T:\R^2\to \R^2</m> is a linear transformation.
        If <m>T\bbm 1\\0\ebm = \bbm 3\\-4\ebm</m> and <m>T\bbm 0\\1\ebm =\bbm 5\\2\ebm</m>,
        find <m>T=\bbm -2\\4\ebm</m>.
      </p>
    </statement>
    <solution>
      <p>
        Since we know the value of <m>T</m> on the standard basis,
        we can use properties of linear transformations to immediately obtain the answer:
        <md>
          <mrow>T\bbm -2\\4\ebm \amp= T\left(-2\bbm 1\\0\ebm +4\bbm 0\\1\ebm\right)</mrow>
          <mrow> \amp = -2T\bbm1\\0\ebm+4T\bbm 0\\1\ebm</mrow>
          <mrow> \amp = -2\bbm 3\\-4\ebm +4\bbm 5\\2\ebm</mrow>
          <mrow> \amp = \bbm 14\\16\ebm</mrow>
        </md>.
      </p>
    </solution>
  </exercise>

  <exercise>
    <statement>
      <p>
        Suppose <m>T:\R^2\to \R^2</m> is a linear transformation.
        Given that <m>T\bbm 3\\1\ebm = \bbm 1\\4\ebm</m>
        and <m>T\bbm 2\\-5\ebm = \bbm 2\\-1\ebm</m>,
        find <m>T\bbm 4\\3\ebm</m>.
      </p>
    </statement>
    <solution>
      <p>
        At first, this example looks the same as the one above,
        and to some extent, it is. The difference is that this time,
        we're given the values of <m>T</m> on a basis that is not the standard one.
        This means we first have to do some work to determine how to write the given vector in terms of the given basis.
      </p>

      <p>
        Suppose we have <m>a\bbm 3\\1\ebm+b\bbm 2\\-5\ebm = \bbm 4\\3\ebm</m>
        for scalars <m>a,b</m>. This is equivalent to the matrix equation
        <me>
          \bbm 3\amp 2\\1\amp -5\ebm\bbm a\\b\ebm = \bbm 4\\3\ebm.
        </me>
        Solving (perhaps using the code cell below), we get <m>a=\frac{26}{17}, b = -\frac{5}{17}</m>.
      </p>

      <sage>
        <input>
          from sympy import Matrix,init_printing
          init_printing()
          A = Matrix(2,2,[3,2,1,-5])
          B = Matrix(2,1,[4,3])
          (A**-1)*B
        </input>
        <output>
          \[\bbm \frac{26}{17}\\-\frac{5}{17}\ebm\]
        </output>
      </sage>

      <p>
        Therefore,
        <me>
          T\bbm 4\\3\ebm = \frac{26}{17}\bbm 1\\4\ebm -\frac{5}{17}\bbm 2\\-1\ebm = \bbm 16/17\\109/17\ebm
        </me>.
      </p>
    </solution>
  </exercise>



  <exercise>
    <statement>
      <p>
        Suppose <m>T:P_2(\R)\to \R</m> is defined by
        <me>
          T(x+2)=1, T(1)=5, T(x^2+x)=0.
        </me>
        Find <m>T(2-x+3x^2)</m>.
      </p>
    </statement>
    <solution>
      <p>
        We need to find scalars <m>a,b,c</m> such that
        <me>
          2-x+3x^2 = a(x+2)+b(1)+c(x^2+x)
        </me>.
        We could set up a system and solve, but this time it's easy enough to just work our way through.
        We must have <m>c=3</m>, to get the correct coefficient for <m>x^2</m>. This gives
        <me>
          2-x+3x^2=a(x+2)+b(1)+3x^2+3x
        </me>.
        Now, we have to have <m>3x+ax=-x</m>, so <m>a=-4</m>.
        Putting this in, we get
        <me>
          2-x+3x^2=-4x-8+b+3x^2+3x
        </me>.
        Simiplifying this leaves us with <m>b=10</m>. Finally, we find:
        <md>
          <mrow>T(2-x+3x^2) \amp = T(-4(x+2)+10(1)+3(x^2+x)) </mrow>
          <mrow> \amp = -4T(x+2)+10T(1)+3T(x^2+x)</mrow>
          <mrow> \amp = -4(1)+10(5)+3(0) = 46</mrow>
        </md>.
      </p>
    </solution>
  </exercise>

  <exercise>
    <statement>
      <p>
        Find a linear transformation <m>T:\R^2\to \R^3</m> such that
        <me>
          T(1,2)=(1,1,0) \quad \text{ and } \quad T(-1,1) = (0,2,-1)
        </me>.
        Then, determine the value of <m>T(3,2)</m>.
      </p>
    </statement>
    <solution>
      <p>
        Since <m>\{(1,2),(-1,1)\}</m> forms a basis of <m>\R^2</m>
        (the vectors are not parallel and there are two of them),
        it suffices to determine how to write a general vector in terms of this basis.
        Suppose
        <me>
          x(1,2)+y(-1,1)=(a,b)
        </me>
        for a general element <m>(a,b)\in \R^2</m>.
        This is equivalent to the matrix equation <m>\bbm 1\amp -1\\2\amp 1\ebm\bbm x\\y\ebm = \bbm a\\b\ebm</m>,
        which we can solve as <m>\bbm x\\y\ebm = \bbm 1\amp -1\\2\amp 1\ebm^{-1}\bbm a\\b\ebm</m>:
      </p>

      <sage>
        <input>
          from sympy import Matrix, init_printing, symbols
          init_printing()
          a, b = symbols('a b', real = True, constant = True)
          A = Matrix(2,2,[1,-1,2,1])
          B = Matrix(2,1,[a,b])
          (A**-1)*B
        </input>
        <output>
          \[\bbm \frac{a}{3}+\frac{b}{3}\\-\frac{2a}{3}+\frac{b}{3}\ebm\]
        </output>
      </sage>

      <p>
        This gives us the result
        <me>
          (a,b) = \frac13(a+b)\cdot (1,2)+\frac13(-2a+b)\cdot (-1,1).
        </me>
        Thus,
        <md>
          <mrow>T(a,b) \amp = \frac13(a+b)\cdot T(1,2)+\frac13(-2a+b)\cdot T(-1,1) </mrow>
          <mrow> \amp = \frac13(a+b)\cdot (1,1,0)+\frac13(-2a+b)\cdot (0,2,-1)</mrow>
          <mrow> \amp = \left(\frac{a+b}{3}, -a+b, \frac{2a-b}{3}\right)</mrow>
        </md>.
      </p>


      <p>
        We conclude that
        <me>
          T(3,2) = \left(\frac53, -1, \frac43\right)
        </me>.
      </p>
    </solution>
  </exercise>



  <exercise xml:id="ex_lintrans-indep">
    <statement>
      <p>
        Let <m>T:V\to W</m> be a linear transformation.
        Prove that for any vectors <m>\vv_1,\ldots, \vv_n\in V</m>,
        if <m>\{T(\vv_1),\ldots, T(\vv_n)\}</m> is linearly independent in <m>W</m>,
        then <m>\{\vv_1,\ldots, \vv_n\}</m> is linearly independent in <m>V</m>.
      </p>
    </statement>
    <solution>
      <p>
        Let us suppose that <m>\{T(\vv_1),\ldots, T(\vv_n)\}</m> is linearly independent.
        We want to show that the set <m>\{\vv_1,\ldots, \vv_n\}</m>
        is linearly independent. To that end, suppose that we have
        <me>
          c_1\vv_1+\cdots + c_n\vv_n=\mathbf{0}
        </me>
        for some scalars <m>c_1,\ldots, c_n</m> (that we want to show must all equal zero).
      </p>

      <p>
        We want to make use of our hypothesis, so we need to bring the linear map <m>T</m> into the picture.
        We apply <m>T</m> to both sides of the equation above, giving us:
        <me>
          T(c_1\vv_1+\cdots + c_n\vv_n)=T(\mathbf{0})
        </me>.
        Now we make use of both parts of <xref ref="thm-lt-props">Theorem</xref> to get
        <me>
          c_1T(\vv_1)+\cdots +c_nT(\vv_n) = \mathbf{0}
        </me>.
        By our hypothesis that the <m>T(\vv_i)</m> are independent,
        we must conclude that all the scalars are zero, and we're done.
      </p>
    </solution>
  </exercise>
</section>
