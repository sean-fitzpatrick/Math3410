<?xml version="1.0" encoding="UTF-8" ?>

<worksheet xml:id="worksheet-eigenvalues">
  <title>Worksheet: Eigenvalues and diagonalization</title>

  <p>
    Let <m>A</m> be a symmetric matrix. According to <xref ref="thm-real-spectral" text="title"/>,
    there exists an orthogonal matrix <m>P</m> such that <m>P^TAP</m> is diagonal,
    with the entries on the diagonal given by the eigenvalues of <m>A</m>.
  </p>

  <p>
    In this worksheet, we will work through the process of finding the eigenvalues of a symmetric matrix,
    along with the orthogonal matrix <m>P</m>.
  </p>

  <p>
    We will also look at the method described in <xref ref="section-matrix-factor"/> for finding dominant eigenvalues.
  </p>

  <exercise label="ex-ws-eigenvalue-1">
    <title>Orthogonal diagonalization</title>
    
    <introduction>
      <p>
        Consider the following <m>5\times 5</m> matrix <m>A</m>:
        <me>
          A = \bbm 97\amp -23 \amp -23\amp -3\amp -46\\
                  -23\amp 49\amp 1\amp 21\amp 2\\
                  -23\amp 1\amp 49\amp 21\amp 2\\
                  -3\amp 21\amp 21\amp 9\amp 42\\
                  -46\amp 2\amp 2\amp 42\amp 52\ebm
        </me>.        
      </p>
    </introduction>
    <task workspace="1in" label="ex-ws-eigenvalue-1a">
      <statement>
        <p>
          First, enter the matrix and confirm that it's symmetric.
          You can compute the transpose of a SymPy matrix <c>A</c> using <c>A.T</c>.
        </p>

        <sage>
          <input>
            import sympy as sy
            sy.init_printing()
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-1b">
      <statement>
        <p>
          Now, find the eigenvalues of <m>A</m>. Yes, we could jump straight to the answer, but let's go through the steps anyway.
          First, compute the characteristic polynomial of <m>A</m>.
          You may with to refer to <xref ref="subsec-sympy-matrix"/> for the correct syntax to use.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          Now, factor the polynomial:
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          Finally, based on the output, input the eigenvalues into the cell below.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-1c">
      <statement>
        <p>
          Recall that if <m>\vv</m> is an eigenvector of <m>A</m> corresponding to the eigenvalue <m>\lambda</m>,
          then <m>\vv</m> belongs to the nullspace of <m>\lambda I-A</m>, where <m>I</m> is the identity matrix.
          You can create an <m>n\times n</m> identity matrix in SymPy using the syntax <c>eye(n)</c>.
        </p>

        <p>
          For each eigenvalue <m>\lambda</m> found in part (b), compute the nullspace of <m>\lambda I-A</m>.
          You will want to refer to these nullspaces later, so give each one a name.
          For example, if your first eigenvalue was 7, you could enter something like
          <cd>
            <cline>E1 = (7*sy.eye(5)-A).nullspace()</cline>
            <cline>E1</cline>
          </cd>
          to get the first eigenspace. Three code cells are provided below.
          If you are in Jupyter, you can add more cells by clicking the <m>+</m> button in the toolbar.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          Finally, let's check our work. Use the command <c>A.eigenvects()</c>
          to compute the eigenvalues and eigenvectors in one step,
          and confirm that the results match what you found above.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-1d">
      <statement>
        <p>
          Next, we want to diagonalize <m>A</m>. There is a <c>diagonalize</c> command,
          but it won't do <em>orthogonal</em> diagonalization, which is what we want.
        </p>

        <p>
          Recall that the eigenvectors corresponding to <em>distinct</em> eigenvalues are automatically orthogonal.
          But this is not true of <em>repeated</em> eigenvalues, 
          and there's a good chance you've found that one of your eigenvalues has multiplicity greater than 1.
        </p>

        <p>
          A matrix <m>P</m> is orthogonal if <m>P^TP=I</m>, in which case te columns of <m>P</m> form an <em>orthonormal</em> basis.
          (So we are looking for an orthonormal basis of eigenvectors.)
        </p>

        <p>
          For any 1-dimensional eigenspaces, you will just need to find a unit eigenvector.
          For example, if <m>\vv = (2,1,0,3,1)</m> is an eigenvector for an eigenvalue of multiplicity 1,
          then you will want to use the eigenvector <m>\uu = \frac{1}{\sqrt{15}}(2,1,0,3,1)</m>, since <m>\len{\vv} = \sqrt{15}</m>.
        </p>

        <p>
          For any higher-dimensional eigenspaces, you can use the Gram-Schmidt algorithm.
          Recall that the optional argument <c>true</c> will produce unit vectors:
          given a basis <c>B</c> for an eigenspace, the command <c>sy.GramSchmidt(B,true)</c> will produce an orthonormal basis.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-1e">
      <statement>
        <p>
          Once you have found five orthonormal eigenvectors create a matrix <m>P</m> whose columns are those eigenvectors:
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          Confirm that your matrix is orthogonal by computing <m>P^TP</m>.
          You should get the identity matrix at the result.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          Finally, compute the matrix <m>P^TAP</m>, and confirm that the result is a diagonal matrix whose entries are the eigenvalues of <m>A</m>.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>
  </exercise>

  <exercise label="ex-ws-eigenvalue-2">
    <introduction>
      <p>
        For the problem you just completed,
        you may have noticed that there was one eigenvalue that was larger in absolute value than all the other eigenvalues. 
        Such an eigenvalue is called a <term>dominant eigenvalue</term>.
      </p>

      <p>
        If you complete <xref ref="worksheet-dynamical"/>, 
        you will see that the state of such systems is ultimately given by a sum of certain geometric series (involving eigenvalues of a matrix), 
        and that the long-term behaviour of the system is approximately geometric, and governed by the dominant eigenvalue.
      </p>

      <p>
        The power method states that, given an initial state vector <m>\xx_0</m>, 
        the sequence of vectors
        <me>
          \xx_0, A\xx_0, A^2\xx_0,\ldots, A^k\xx_0,\ldots
        </me>
        will converge to an eigenvector for the dominant eigenvalue.
      </p>
    </introduction>

    <task workspace="1in" label="ex-ws-eigenvalue-2a">
      <statement>
        <p>
          Let's see if this is true. Below, you are given an initial guess <c>x0</c> and an empty list <c>L</c>. 
          You want to populate <c>L</c> with vectors of the form <m>A^k\xx_0</m>, 
          for <m>0\leq k\leq 10</m>. This is most easily done using a <c>for</c> loop. 
          For details on syntax, see <xref ref="pars-power-method"/>
        </p>

        <sage>
          <input>
            x0 = sy.Matrix([1,0,1,0,1])
            L = list()
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          Let's print the last vector in the list, which will probably have some pretty big entries:
        </p>

        <sage>
          <input>
            L[10]
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-2b">
      <statement>
        <p>
          Now let's check our work. How can we tell if this vector is (approximately) a multiple of the dominant eigenvector? 
          One option is to divide each entry in <c>L[10]</c> by the smallest (in absolute value) non-zero entry in <c>L[10]</c>. 
          How does this compare to the eigenvector you originally found for the dominant eigenvalue?
        </p>

        <p>
          If you find that some entries look right, but not others, see the last part of the worksheet.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-2c">
      <statement>
        <p>
          Finally, suppose we didn't know what the dominant eigenvalue was, and we wanted to find it.
          Note that if <m>\xx</m> is a dominant eigenvector, 
          then <m>A\xx = \lambda\xx</m>, where <m>\lambda</m> is the dominant eigenvalue. Then
          <me>
            \lambda = \frac{\xx\cdot (\lambda\xx)}{\xx\cdot \xx} = \frac{\xx\cdot A\xx}{\xx\cdot \xx}
          </me>.
        </p>

        <p>
          It seems reasonable, then, that if <m>\xx_k</m> is <em>approximately</em> equal to a dominant eigenvector, then the numbers
          <me>
            r_k = \frac{\xx_k\cdot A\xx_k}{\xx_k\cdot \xx_k}=\frac{\xx_k\cdot \xx_{k+1}}{\lVert \xx_k\rVert^2}
          </me>
          should be approximately equal to the dominant eigenvalue. (These numbers are the so-called <term>Rayliegh quotients</term>.)
        </p>

        <p>
          Following the approach in <xref ref="pars-power-method"/>, 
          compute the Rayleigh quotients <m>r_k</m> for <m>1\leq k\leq 10</m>, 
          and comment on how well they approximate the dominant eigenvalue of <m>A</m>.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>

    <task workspace="1in" label="ex-ws-eigenvalue-2d">
      <statement>
        <p>
          Does it look like the numbers are approaching the dominant eigenvalue? Or do they seem to be getting near one of the other eigenvalues?
        </p>

        <p>
          If your numbers seem wrong, it might be for the following reason: 
          when we write our initial guess <c>x0</c> as a linear combination of the eigenvectors, 
          the coefficient of the dominant eigenvector has to be nonzero. Is that the case here? 
          To check, note that if <m>\vec{c}</m> is the vector of coefficients, 
          then we must have <m>P\vec{c}=\xx_0</m>. Can you solve this equation for <m>\vec{c}</m>?
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>

        <p>
          If the first entry in <c>c</c> is zero, enter a new value for <m>x0</m>,
          and try the above steps again in the cell below.
        </p>

        <sage>
          <input>
            
          </input>
          <output>
            
          </output>
        </sage>
      </statement>
    </task>
  </exercise>
</worksheet>
