<?xml version="1.0" encoding="UTF-8" ?>

<section xml:id="section-matrix-factor">
  <title>Matrix Factorizations and Eigenvalues</title>
  <introduction>
    <p>
      This section is a rather rapid tour of some cool ideas that get a lot of use in applied linear algebra.
      We are rather light on details here. The interested reader can consult sections 8.3<ndash/>8.6 in
      the Nicholson textbook.
    </p>
  </introduction>

  <subsection xml:id="subsec-matrix-factorization">
    <title>Matrix Factorizations</title>
      <subsubsection xml:id="pars-positive-ops">
      <title>Positive Operators</title>
      <p>
        Let <m>T</m> be a linear operator defined by a matrix <m>A</m>.
        If <m>A</m> is symmetric (for the case of <m>\R^n</m>)
        or hermitian (for the case of <m>\C^n</m>),
        we say that the operator <m>T</m> is <term>self-adjoint</term>.
      </p>
      <definition xml:id="def-positive-op">
        <statement>
          <p>
            A self-adjoint operator <m>T</m> is <term>positive</term>
            if <m>\xx^H T\xx\geq 0</m> for all vectors <m>\xx\neq \zer</m>.
            It is <term>positive-definite</term> if <m>\xx^H T\xx\gt 0</m> for all nonzero <m>\xx</m>.
            If <m>T=T_A</m> for some matrix <m>A</m>,
            we also refer to <m>A</m> as a positive(-definite) matrix.
          </p>
        </statement>
      </definition>

      <aside>
        <p>
          Some books will define positive-definite operators by the condition <m>\xx^H T\xx\geq 0</m>
          without the requirement that <m>T</m> is self-adjoint.
          However, we will stick to the simpler definition.
        </p>
      </aside>

      <p>
        The definition of a positive matrix is equivalent to requiring that all its eigenvalues are non-negative.
        Every positive matrix <m>A</m> has a unique positive square root: a matrix <m>R</m> such that <m>R^2=A</m>.
        Since <m>A</m> is symmetric/hermitian, it can be diagonalized.
        Writing <m>A = PDP^{-1}</m> where <m>P</m> is orthogonal/unitary and
        <me>
          D = \bbm \lambda_1 \amp 0\amp \cdots \amp 0\\0\amp \lambda_2 \amp \cdots \amp 0\\ \vdots \amp \vdots \amp \ddots \amp \vdots\\ 0\amp 0\amp \cdots \amp \lambda_n\ebm
        </me>,
        we have <m>R=PEP^{-1}</m>, where
        <me>
          D = \bbm \sqrt{\lambda_1} \amp 0\amp \cdots \amp 0\\0\amp \sqrt{\lambda_2} \amp \cdots \amp 0\\ \vdots \amp \vdots \amp \ddots \amp \vdots\\ 0\amp 0\amp \cdots \amp \sqrt{\lambda_n}\ebm
        </me>.
      </p>

      <p>
        The following theorem gives us a simple way of generating positive matrices.
      </p>

      <theorem xml:id="thm-positive-prod">
        <statement>
          <p>
            For any <m>n\times n</m> matrix <m>U</m>, the matrix <m>A=U^TU</m> is positive.
            Moreover, if <m>U</m> is invertible, then <m>A</m> is positive-definite.
          </p>
        </statement>
        <proof>
          <p>
            For any <m>\xx\neq \zer</m> in <m>\R^n</m>,
            <me>
              \xx^T A\xx = \xx^TU^T U\xx = (U\xx)^T(U\xx) = \len{U\xx}^2\geq 0
            </me>.
          </p>
        </proof>
      </theorem>

      <p>
        What is interesting is that the converse to the above statement is also true.
        The <term>Cholesky factorization</term> of a positive-definite matrix <m>A</m>
        is given by <m>A=U^TU</m>, where <m>U</m> is upper-triangular,
        with positive diagonal entries.
      </p>

      <p>
        Even better is that there is a very simple algorithm for obtaining the factorization:
        Carry the matrix <m>A</m> to triangular form, using only row operations of the type
        <m>R_i+kR_j\to R_i</m>. Then divide each row by the square root of the diagonal entry
        to obtain the matrix <m>U</m>.
      </p>

      <p>
        The SymPy library contains the <c>cholesky()</c> algorithm.
        Note however that it produces a lower triangular matrix, rather than upper triangular.
        (That is, the output gives <m>L=U^T</m> rather than <m>U</m>, so you will have <m>A=LL^T</m>.)
        Let's give it a try. First, let's create a positive-definite matrix.
      </p>

      <sage>
        <input>
          from sympy import Matrix,init_printing
          init_printing()
          B = Matrix([[3,7,-4],[5,-9,2],[-3,0,6]])
          A = B*B.T
          A
        </input>
        <output>
          \[\bbm 74\amp -56\amp -33\\-56\amp 110\amp -3\\-33\amp -3\amp 45\ebm\]
        </output>
      </sage>

      <p>
        Next, find the Cholesky factorization:
      </p>

      <sage>
        <input>
          L = A.cholesky()
          L, L*L.T
        </input>
        <output>
          \[\left(\bbm \sqrt{10}\amp 0\amp 0\\ \frac{\sqrt{10}}{2} \amp \frac{\sqrt{2}}{2} \amp 0\\\frac{\sqrt{10}}{5} \amp \sqrt{2}\amp \frac{\sqrt{15}}{5}\ebm, \bbm 10\amp 5\amp 2\\ 5\amp 3\amp 2\\ 2\amp 2\amp 3\ebm\right)\]
        </output>
      </sage>

      <sage>
        <input>
          L*L.T == A
        </input>
        <output>
          True
        </output>
      </sage>

      <p>
        Note that <m>L</m> is <em>not</em> the same as the matrix <m>B</m>!
      </p>
    </subsubsection>

    <subsubsection xml:id="pars-singular-values">
      <title>Singular Value Decomposition</title>
      <p>
        For any <m>n\times n</m> matrix <m>A</m>, the matrices <m>A^TA</m> and <m>AA^T</m> are both positive. (Exercise!)
        This means that we can define <m>\sqrt{A^TA}</m>, even if <m>A</m> itself is not symmetric or positive.
      </p>

      <p>
        <ul>
          <li>
            <p>
              Since <m>A^TA</m> is symmetric, we know that it can be diagonalized.
            </p>
          </li>
          <li>
            <p>
              Since <m>A^TA</m> is positive, we know its eigenvalues are non-negative.
            </p>
          </li>
          <li>
            <p>
              This means we can define the <term>singular values</term> <m>\sigma_i = \sqrt{\lambda_i}</m> for each <m>i=1,\ldots, n</m>.
            </p>
          </li>
          <li>
            <p>
              <alert>Note:</alert> it's possible to do this even if <m>A</m> is not a square matrix!
            </p>
          </li>
        </ul>
      </p>

      <p>
        The SymPy library has a function for computing the singular values of a matrix.
        Given a matrix <c>A</c>, the command <c>A.singular_values()</c> will return its singular values.
        Try this for a few different matrices below:
      </p>

      <sage>
        <input>
          A = Matrix([[1,2,3],[4,5,6]])
          A.singular_values()
        </input>
        <output>
          \[\left[\sqrt{\frac{\sqrt{8065}}{2}+\frac{91}{2}},\sqrt{\frac{91}{2}-\frac{\sqrt{8065}}{2}},0\right]\]
        </output>
      </sage>

      <p>
        In fact, SymPy can even return singular values for a matrix with variable entries!
        Try the following example from the <url href="https://docs.sympy.org/latest/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixEigen.singular_values" visual="docs.sympy.org/latest/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixEigen.singular_values">SymPy documentation</url>.
      </p>

      <sage>
        <input>
          from sympy import Symbol
          x = Symbol('x', real=True)
          M = Matrix([[0,1,0],[0,x,0],[-1,0,0]])
          M,M.singular_values()
        </input>
        <output>
          \[\left(\bbm 0\amp 1\amp 0\\ 0\amp x\amp 0\\ -1\amp 0\amp 0\ebm , \left[ \sqrt{x^2+1}, 1, 0\right]\right)\]
        </output>
      </sage>

      <p>
        For an <m>n\times n</m> matrix <m>A</m>, we might not be able to diagonalize <m>A</m>
        (with a single orthonormal basis).
        However, it turns out that it's <em>always</em> possible to find a pair of orthonormal bases
        <m>\{e_1,\ldots, e_n\}, \{f_1,\ldots, f_n\}</m> such that
        <me>
          Ax = \sigma_1(x\cdot e_1)f_1+\cdots + \sigma_n(x\cdot e_n)f_n
        </me>.
        In matrix form, <m>A = P\Sigma_A Q^T</m> for orthogonal matrices <m>P,Q</m>.
      </p>

      <p>
        In fact, this can be done even if <m>A</m> is not square,
        which is arguably the more interesting case! Let <m>A</m> be an <m>m\times n</m> matrix.
        We will find an <m>m\times m</m> orthogonal matrix <m>P</m> and <m>n\times n</m> orthogonal matrix <m>Q</m>,
        such that <m>A=P\Sigma_A Q^T</m>, where <m>\Sigma_A</m> is also <m>m\times n</m>.
      </p>

      <aside>
        <p>
          If <m>A</m> is symmetric and positive-definite,
          the singular values of <m>A</m> are just the eigenvalues of <m>A</m>,
          and the singular value decomposition is the same as diagonalization.
        </p>
      </aside>

      <p>
        The basis <m>\{f_1,\ldots, f_n\}</m> is an orthonormal basis for <m>A^TA</m>,
        and the matrix <m>Q</m> is the matrix whose columns are the vectors <m>f_i</m>.
        As a result, <m>Q</m> is orthogonal.
      </p>

      <p>
        The matrix <m>\Sigma_A</m> is the same size as <m>A</m>.
        First, we list the positive singular values of <m>A</m> in decreasing order:
        <me>
          \sigma_1\geq \sigma_2\geq \cdots \geq \sigma_k\gt 0
        </me>.
        Then, we let <m>D_A = \operatorname{diag}(\sigma_1,\ldots, \sigma_k)</m>,
        and set
        <me>
          \Sigma_A = \begin{bmatrix}D_A\amp 0\\0\amp 0\end{bmatrix}
        </me>.
        That is, we put <m>D_A</m> in the upper-left, and then fill in zeros as needed,
        until <m>\Sigma_A</m> is the same size as <m>A</m>.
      </p>

      <p>
        Next, we compute the vectors <m>e_i = \frac{1}{\len{Af_i}}Af_i</m>, for <m>i=1,\ldots, k</m>.
        As shown in Nicolson, <m>\{e_1,\ldots, e_r\}</m> will be an orthonormal basis for the column space of <m>A</m>.
        The matrix <m>P</m> is constructed by extending this to an orthonormal basis of <m>\R^m</m>.
      </p>

      <p>
        All of this is a lot of work to do by hand, but it turns out that it can be done numerically,
        and more importantly, <em>efficiently</em>, by a computer.
        The SymPy library has an <init>SVD</init> algorithm,
        but it will not be efficient for larger matrices.
        In practice, most Python users will use the <init>SVD</init> algorithm provided by NumPy;
        we will stick with SymPy for simplicity and consistency.
      </p>

      <remark>        
        <p>
          The version of the <init>SVD</init> given above is not used in computations,
          since it tends to be more resource intensive.
          In particular, it requires us to store more information than necessary:
          the last <m>n-r</m> rows of <m>Q</m>, and the last <m>m-r</m> columns of <m>P</m>,
          get multiplied by columns/rows of zeros in <m>\Sigma_A</m>,
          so we don't really need to keep track of these columns.
        </p>

        <p>
          Instead, most algorithms that you find will give the <m>r\times r</m> diagonal matrix <m>D_A</m>,
          consisting of the nonzero singular values,
          and <m>P</m> will be replaced by the <m>m\times r</m> matrix consisting of its first <m>r</m> columns,
          while <m>Q</m> gets replaced by the <m>r\times n</m> matrix consisting of its first <m>r</m> rows.
          The resulting product is still equal to the original matrix.
        </p>

        <p>
          In some cases, even the matrix <m>D_A</m> is too large,
          and a decision is made to truncate to some smaller subset of singular values.
          In this case, the resulting product is no longer equal to the original matrix,
          but it does provide an approximation. A discussion can be found
          <url href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs" visual="en.wikipedia.org/wiki/Singular_value_decomposition">on Wikipedia</url>.
        </p>
      </remark>

      <example xml:id="example-svd">
        <statement>
          <p>
            Find the singular value decomposition of the matrix <m>A = \begin{bmatrix}1\amp 1\amp 1\\1\amp 0\amp -1\end{bmatrix}</m>.
          </p>
        </statement>
        <solution>
          <p>
            Using SymPy, we get the <url href="https://docs.sympy.org/latest/modules/matrices/matrices.html#sympy.matrices.matrices.MatrixBase.singular_value_decomposition" visual="docs.sympy.org/latest/modules/matrices/">condensed SVD</url>.
            First, let's check the singular values.
          </p>

          <sage>
            <input>
              from sympy import Matrix, init_printing
              init_printing()
              A = Matrix([[1,1,1],[1,0,-1]])
              A.singular_values()
            </input>
            <output>
              \[\left[\sqrt{2},\sqrt{3},0\right]\]
            </output>
          </sage>

          <p>
             Note that the values are not listed in decreasing order.
             Now, let's ask for the singular value decomposition.
             The output consists of three matrices;
             the first line below assigns those matrices to the names <c>P,S,Q</c>.
          </p>

          <sage>
            <input>
              P,S,Q=A.singular_value_decomposition()
              P,S,Q
            </input>
            <output>
              \[\left(\bbm 0\amp 1\\1\amp 0\ebm, \bbm \sqrt{2}amp 0\\0\sqrt{3}\ebm, \bbm \frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\\0\amp \frac{\sqrt{3}}{3}\\ -\frac{\sqrt{2}}{2}\amp \frac{\sqrt{3}}{3}\ebm\right)\]
            </output>
          </sage>

          <p>
            Note that the output is the <q>condensed</q> version, which doesn't match the exposition above.
            It also doesn't follow the same ordering convention: we'll need to swap columns in each of the matrices.
            But it does give us a decomposition of the matrix <m>A</m>:
          </p>

          <sage>
            <input>
              P*S*Q.T
            </input>
            <output>
              \[\bbm 1\amp 1\amp 1\\1\amp 0\amp -1\ebm\]
            </output>
          </sage>
          <p>
            To match our earlier presentation, we first set <m>\Sigma_A = \bbm \sqrt{3}\amp 0\amp 0\\0 \amp \sqrt{2}\amp 0\ebm</m>.
            Next, we need to extend the <m>3\times 2</m> matrix in the output above to a <m>3\times 3</m> matrix.
            We can do this by choosing any vector orthogonal to the two existing columns, and normalizing.
            Let's use entries <m>1/\sqrt{6},-2/\sqrt{6},1/\sqrt{6}</m>.
            Noting that we also need to swap the first two columns (to match the fact that we swapped columns in <m>\Sigma_A</m>),
            we get the matrix
            <me>
              Q = \bbm \frac{\sqrt{3}}{3}\amp \frac{\sqrt{2}}{2}\amp \frac{\sqrt{6}}{6}\\
                       \frac{\sqrt{3}}{3}\amp 0\amp -\frac{\sqrt{6}}{3}\\
                       \frac{\sqrt{3}}{3}\amp -\frac{\sqrt{2}}{2} \amp \frac{\sqrt{6}}{6}\ebm
            </me>.
            Let's check that it is indeed orthogonal.
          </p>

          <sage>
            <input>
              Q = Matrix([
                  [sqrt(3)/3,sqrt(2)/2,sqrt(6)/6],
                  [sqrt(3)/3,0,-sqrt(6)/3],
                  [sqrt(3)/3,-sqrt(2)/2,sqrt(6)/6]])
              Q*Q.T
            </input>
            <output>
              \[\bbm 1\amp 0\amp 0\\0\amp 1\amp 0\\ 0\amp 0\amp 1\ebm\]
            </output>
          </sage>

          <p>
            Finally, we take <m>P=\bbm 1\amp 0\\0\amp 1\ebm</m> (again swapping columns),
            which is just the identity matrix. We therefore should expect that
            <me>
              P\Sigma_A Q^T = \Sigma_A Q^T = A
            </me>.
            Let's check.
          </p>

          <sage>
            <input>
              S = Matrix([[sqrt(3),0,0],[0,sqrt(2),0]])
              S*Q.T
            </input>
            <output>
              \[\bbm 1\amp 1\amp 1\\1\amp 0\amp -1\ebm\]
            </output>
          </sage>

          <p>
            It worked!
          </p>
        </solution>
      </example>

      <p>
        The Singular Value Decomposition has a lot of useful appplications,
        some of which are described in Nicholson's book.
        On a very fundamental level the <init>SVD</init> provides us with information on some of the most essential properties of the matrix <m>A</m>,
        and any system of equations with <m>A</m> as its coefficient matrix.
      </p>

      <p>
        Recall the following definitions for an <m>m\times n</m> matrix <m>A</m>:
        <ol>
          <li>
            <p>
              The <term>rank</term> of <m>A</m> is the number of leadning ones in the <init>RREF</init> of <m>A</m>,
              which is also equal to the dimension of the column space of <m>A</m> (or if you prefer, the dimension of <m>\im (T_A)</m>).
            </p>
          </li>
          <li>
            <p>
              The <term>column space</term> of <m>A</m>, denoted <m>\csp(A)</m>,
              is the subspace of <m>\R^m</m> spanned by the columns of <m>A</m>.
              (This is the image of the matrix transformation <m>T_A</m>;
              it is also the space of all vectors <m>\mathbf{b}</m> for which the system <m>A\xx=\mathbf{b}</m> is consistent.)
            </p>
          </li>
          <li>
            <p>
              The <term>row space</term> of <m>A</m>, denoted <m>\operatorname{row}(A)</m>, is the span of the rows of <m>A</m>,
              viewed as column vectors in <m>\R^n</m>.
            </p>
          </li>
          <li>
            <p>
              The <term>null space</term> of <m>A</m> is the space of solutions to the homogeneous system <m>A\xx=\zer</m>.
              This is, of course, equal the kernel of the associated transformation <m>T_A</m>.
            </p>
          </li>
        </ol>
      </p>

      <p>
        There are some interesting relationships among these spaces,
        which are left as an exercise.
      </p>

      <exercise xml:id="ex-matrix-factor1" label="ex-matrix-factor1">
        <introduction>
          <p>
            Let <m>A</m> be an <m>m\times n</m> matrix. Prove the following statements.
          </p>
        </introduction>
        <task label="ex-mf1a">
          <statement>
            <p>
              <m>(\operatorname{row}(A))^\bot = \nll(A)</m>
            </p>
          </statement>
          <response/>
          <hint>
            <p>
              Note that <m>\vv\in \nll(A)</m> if and only if <m>A\vv=\zer</m>,
              and <m>\vv\in(\operatorname{row}(A))^\bot</m> if and only if <m>\vv\cdot \mathbf{r}_i=\zer</m>
              for each row <m>\mathbf{r}_i</m> of <m>A</m>.
            </p>

            <p>
              Note also that <m>(A\vv)^T=\vv^T A^T</m> is the (dot) product of <m>\vv^T</m>
              with each column of <m>A^T</m>, and each column of <m>A^T</m> is a row of <m>A</m>.
            </p>
          </hint>
        </task>
        <task label="ex-mf1b">
          <statement>
            <p>
              <m>(\csp(A))^\bot = \nll(A^T)</m>
            </p>
          </statement>
          <response/>
          <hint>
            <p>
              Notice that <m>\vv\in \nll(A^T)</m> if and only if <m>A^T\vv=\zer</m>,
              and that <m>(A^T\vv)^T=\vv^T A</m>.
              Your reasoning should be similar to that of the previous part.
            </p>
          </hint>
        </task>
      </exercise>

      <p>
        Here's the cool thing about the <init>SVD</init>.
        Let <m>\sigma_1\geq \sigma_2\geq \cdots \geq \sigma_r\gt 0</m> be the positive singular values of <m>A</m>.
        Let <m>\vecq_1,\ldots, \vecq_r,\ldots, \vecq_n</m> be the orthonormal basis of eigenvectors for <m>A^TA</m>,
        and let <m>\vecp_1,\ldots, \vecp_r,\ldots, \vecp_m</m>
        be the orthonormal basis of <m>\R^m</m> constructed in the <init>SVD</init> algorithm. Then:
      </p>

      <p>
        <ol>
          <li>
            <p>
              <m>\rank(A)=r</m>
            </p>
          </li>
          <li>
            <p>
              <m>\vecq_1,\ldots, \vecq_r</m> form a basis for <m>\operatorname{row}(A)</m>.
            </p>
          </li>
          <li>
            <p>
              <m>\vecp_1,\ldots, \vecp_r</m> form a basis for <m>\csp(A)</m>
              (and thus, the <q>row rank</q> and <q>column rank</q> of <m>A</m> are the same).
            </p>
          </li>
          <li>
            <p>
              <m>\vecq_{r+1},\ldots, \vecq_n</m> form a basis for <m>\nll(A)</m>.
              (And these are therefore the basis solutions of <m>A\xx=\zer</m>!)
            </p>
          </li>
          <li>
            <p>
              <m>\vecp_{r+1},\ldots, \vecp_m</m> form a basis for <m>\nll(A^T)</m>.
            </p>
          </li>
        </ol>
      </p>

      <p>
        If you want to explore this further, have a look at the excellent
        <url href="https://www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html" visual="www.juanklopper.com/wp-content/uploads/2015/03/III_05_Singular_value_decomposition.html">notebook by Dr. Juan H Klopper</url>.
        The <c>ipynb</c> file can be found <url href="https://github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06" visual="github.com/juanklopper/MIT_OCW_Linear_Algebra_18_06">on his GitHub page</url>.
        In it, he takes you through various approaches to finding the singular value decomposition,
        using the method above, as well as using NumPy and SciPy (which, for industrial applications, are superior to SymPy).
      </p>
    </subsubsection>

    <subsubsection xml:id="pars-qr-factor">
      <title>QR Factorization</title>
      <p>
        Suppose <m>A</m> is an <m>m\times n</m> matrix with independent columns.
        (Question: for this to happen, which is true <mdash/> <m>m\geq n</m>, or <m>n\geq m</m>?)
      </p>

      <p>
        A <m>QR</m>-factorization of <m>A</m> is a factorization of the form <m>A=QR</m>,
        where <m>Q</m> is <m>m\times n</m>, with orthonormal columns,
        and <m>R</m> is an invertible upper-triangular (<m>n\times n</m>) matrix with positive diagonal entries.
        If <m>A</m> is a square matrix, <m>Q</m> will be orthogonal.
      </p>

      <p>
        A lot of the methods we're looking at here involve more sophisticated numerical techniques than SymPy is designed to handle.
        If we wanted to spend time on these topics, we'd have to learn a bit about the NumPy package,
        which has built in tools for finding things like polar decomposition and singular value decomposition.
        However, SymPy does know how to do <m>QR</m> factorization. After defining a matrix <c>A</c>, we can use the command
        <cd>
          Q, R = A.QRdecomposition()
        </cd>.
      </p>

      <sage>
        <input>
          from sympy import Matrix,init_printing
          init_printing()
          A = Matrix(3,3,[1,-2,3,3,-1,2,4,2,5])
          Q, R = A.QRdecomposition()
          A, Q, R
        </input>
        <output>
          \[\left(\bbm 1\amp -2\amp 3\\ 3\amp -1\amp 2\\ 4\amp 2\amp 5\ebm, \bbm \frac{\sqrt{26}}{6} \amp -\frac{11\sqrt{26}}{78} \amp \frac23\\ \frac{3\sqrt{26}}{26} \amp -\frac{7\sqrt{26}}{78} \amp -\frac23\\ \frac{2\sqrt{26}}{13} \amp \frac{4\sqrt{26}}{39} \amp \frac13\ebm, \bbm \sqrt{26}\amp \frac{3\sqrt{26}}{26} \amp \frac{29\sqrt{26}}{26}\\ 0\amp \frac{15\sqrt{26}}{26} \amp -\frac{7\sqrt{26}}{78}\\0\amp 0\amp \frac73\ebm\right)\]
        </output>
      </sage>

      <p>
        Let's check that the matrix <m>Q</m> really is orthogonal:
      </p>
      <sage>
        <input>
          Q**(-1) == Q.T
        </input>
        <output>
          True
        </output>
      </sage>

      <p>
        Details of how to perform the QR factorization can be found in Nicholson's textbook.
        It's essentially a consequence of performing the Gram-Schmidt algorithm on the columns of <m>A</m>,
        and keeping track of our work.
      </p>

      <p>
        The calculation above is a symbolic computation, which is nice for understanding what's going on.
        The reason why the <m>QR</m> factorization is useful in practice is that there are efficient numerical methods for doing it
        (with good control over rounding errors).
        Our next topic looks at a useful application of the <m>QR</m> factorization.
      </p>
    </subsubsection>
  </subsection>

  <subsection xml:id="subsec-compute-eigen">
    <title>Computing Eigenvalues</title>
    <introduction>
      <p>
        Our first method focuses on the dominant eigenvalue of a matrix.
        An eigenvalue is dominant if it is larger in absolute value than all other eigenvalues.
        For example, if <m>A</m> has eigenvalues <m>1,3,-2,-5</m>, then <m>-5</m> is the dominant eigenvalue.
      </p>

      <p>
        If <m>A</m> has eigenvalues <m>1,3,0,-4,4</m> then there is no dominant eigenvalue.
        Any eigenvector corresponding to a dominant eigenvalue is called a dominant eigenvector.
      </p>
    </introduction>

    <subsubsection xml:id="pars-power-method">
      <title>The Power Method</title>
      <p>
        If a matrix <m>A</m> has a dominant eigenvalue,
        there is a method for finding it (approximately)
        that does not involve finding and factoring the characteristic polynomial of <m>A</m>.
      </p>

      <p>
        We start with some initial guess <m>\xx_0</m> for a dominant eigenvector.
        We then set <m>\xx_{k+1} = A\xx_k</m> for each <m>k\geq 0</m>, giving a sequence
        <me>
          \xx_0, A\xx_0, A^2\xx_0, A^3\xx_0,\ldots
        </me>.
        We expect (for reasons we'll explain) that <m>\lVert \xx_k-\xx\rVert \to 0</m> as <m>k\to\infty</m>,
        where <m>\xx</m> is a dominant eigenvector. Let's try an example.
      </p>

      <sage>
        <input>
          A = Matrix(2,2,[1,-4,-3,5])
          A,A.eigenvects()
        </input>
        <output>
          \[\left(\bbm 1\amp -4\\-3\amp 5\ebm, \left[\left(-1, 1, \left[\bbm 2\\1\ebm\right]\right),\left(7, 1, \left[\bbm -\frac23\\1\ebm\right]\right)\right]\right)\]
        </output>
      </sage>

      <p>
        The dominant eigenvalue is <m>\lambda = 7</m>. Let's try an initial guess of
        <m>\xx_0=\begin{bmatrix}1\\0\end{bmatrix}</m> and see what happens.
      </p>

      <sage>
        <input>
          x0 = Matrix(2,1,[1,0])
          L = list()
          for k in range(10):
              L.append(A**k*x0)
          L
        </input>
        <output>
          \begin{align*}
          \left[\bbm 1\\0\ebm, \bbm 1\\-3\ebm,\right. \amp \bbm 13\\-18\ebm, \bbm 85\\-129\ebm, \bbm 601\\-900\ebm, \bbm 4201\\-6303\ebm,\\ \amp \left.\bbm 29413\\ -44118\ebm, \bbm 205885\\-308829\ebm, \bbm 1441201\\-2161800\ebm, \bbm 10088401\\-15132603\ebm\right]
          \end{align*}
        </output>
      </sage>

      <p>
        We might want to confirm whether that rather large fraction is close to <m>\frac23</m>.
        To do so, we can get the computer to divide the numerator by the denominator.
      </p>

      <sage>
        <input>
          L[9][0]/L[9][1]
        </input>
        <output>
          \[-\frac{10088401}{15132603}, \text{ or } -0.666666600584182\]
        </output>
      </sage>

      <p>
        The above might show you the fraction rather than its decimal approximation.
        (This may depend on whether you're on Sage or Jupyter.)
        To get the decimal, try wrapping the above in <c>float()</c> (or <c>N</c>, or append with <c>.evalf()</c>).
      </p>

      <p>
        For the eigenvalue, we note that if <m>A\xx=\lambda \xx</m>, then
        <me>
          \frac{\xx\cdot A\xx}{\lVert \xx\rVert^2} = \frac{\xx\cdot (\lambda x)}{\lVert \xx\rVert^2} = \lambda
        </me>.
        This leads us to consider the Rayleigh quotients
        <me>
          r_k = \frac{\xx_k\cdot \xx_{k+1}}{\lVert \xx_k\rVert^2}
        </me>.
      </p>

      <sage>
        <input>
          M = list()
          for k in range(9):
              M.append((L[k].dot(L[k+1]))/(L[k].dot(L[k])))
          M
        </input>
        <output>
          \begin{align*}
          \left[1,\frac{67}{10}, \frac{3427}{493}, \frac{167185}{23866}\right.,\amp\frac{8197501}{1171201},\frac{401639767}{57376210},\\ \amp \left.\frac{19680613327}{2811522493},\frac{964348200085}{137763984466},\frac{47253074775001}{6750439562401}\right]
          \end{align*}
        </output>
      </sage>

      <p>
        We can convert a rational number r to a float using either <c>N(r)</c> or <c>r.evalf()</c>.
        (The latter seems to be the better bet when working with a list.)
      </p>

      <sage>
        <input>
          M2 = list()
          for k in range(9):
              M2.append((M[k]).evalf())
          M2
        </input>
        <output>
          \begin{align*}
          1.0,\amp 6.7,6.95131845841785,\\ \amp 7.00515377524512, 6.99922643508672, 7.00010974931945,\\ \amp \qquad \left. 6.9999843060121, 7.00000224168168, 6.9999996797533\right]
          \end{align*}
        </output>
      </sage>

      <p>
        So why does this work? Suppose <m>A</m> is diagonalizable,
        so that there is a basis of eigenvectors <m>B = \{\vv_1,\ldots, \vv_n\}</m> for <m>\R^n</m>.
        Suppose also that <m>A</m> has dominant eigenvalue <m>\lambda_1</m>,
        with corresponding eigenvector <m>\vv_1</m>.
        Then our initial guess <m>\xx_0</m> can be written in terms of this basis:
        <me>
          \xx_0 = c_1\vv_1+c_2\vv_2+\cdots + c_n\vv_n
        </me>,
        for scalars <m>c_1,c_2,\ldots, c_n</m>.
      </p>

      <p>
        For each natural number <m>k</m> we have
        <md>
          <mrow>A^k\xx_0 \amp = c_1A^k\vv_1 + c_2A^k\vv_2 + \cdots + c_nA^k\vv_n</mrow>
          <mrow>\xx_k \amp = c_1\lambda_1^k\vv_1+c_2\lambda_2^k\vv_2+\cdots + c_n\lambda_n^k\vv_n</mrow>
        </md>.
        Dividing by <m>\lambda_1^k</m> we get
        <me>
          \frac{1}{\lambda_1^k}\xx_k = c_1\vv_1 + c_2\left(\frac{\lambda_2}{\lambda_1}\right)^k\vv_2+\cdots + c_n\left(\frac{\lambda_n}{\lambda_1}\right)^k
        </me>,
        and since <m>\lambda_1</m> is larger than all the other eigenvalues in absolute value,
        the powers <m>\left(\frac{\lambda_m}{\lambda_1}\right)^k</m> will be very small when <m>k</m> is large.
      </p>

      <p>
        If <m>\vv_1</m> is an eigenvector corresponding to the dominant eigenvalue <m>\lambda_1</m>,
        then so is the scalar multiple <m>c_1\lambda_1^k\vv_1</m>, so <m>\xx_k</m> is approximately equal to an eigenvector.
      </p>

      <p>
        Two things are worth noting:
        <ol>
          <li>
            <p>
              The result does not depend on the initial guess <m>\xx_0</m>,
              unless we choose an <m>\xx_0</m> for which the coefficient <m>c_1</m> is 0.
              If this turns out to be the case, we must choose another initial guess.
            </p>
          </li>
          
          <li>
            <p>
              If <m>A</m> is not diagonalizable, we can still use this method,
              but we must choose <m>\xx_0</m> from the subspace spanned by a set of eigenvectors for <m>A</m>.
            </p>
          </li>
        </ol>
      </p>
    </subsubsection>

    <subsubsection xml:id="pars-qr-algorithm">
      <title>The QR Algorithm</title>
      <p>
        Given an <m>n\times n</m> matrix <m>A</m>, we know we can write <m>A=QR</m>,
        with <m>Q</m> orthogonal and <m>R</m> upper-triangular.
        The <m>QR</m>-algorithm exploits this fact. We set <m>A_1=A</m>, and write <m>A_1=Q_1R_1</m>.
      </p>

      <p>
        Then we set <m>A_2 = R_1Q_1</m>, and factor: <m>A_2=Q_2R_2</m>.
        Notice <m>A_2 = R_1Q_1 = Q_1^TA_1Q_1</m>. Since <m>A_2</m> is similar to <m>A_1</m>,
        <m>A_2</m> has the same eigenvalues as <m>A_1=A</m>.
      </p>

      <p>
        Next, set <m>A_3 = R_2Q_2</m>, and factor as <m>A_3 = Q_3R_3</m>.
        Since <m>A_3 = Q_2^TA_2Q_2</m>, <m>A_3</m> has the same eigenvalues as <m>A_2</m>.
        In fact, <m>A_3 = Q_2^T(Q_1^TAQ_1)Q_2 = (Q_1Q_2)^TA(Q_1Q_2)</m>.
      </p>

      <p>
        After <m>k</m> steps we have <m>A_{k+1} = (Q_1\cdots Q_k)^TA(Q_1\cdots Q_k)</m>,
        which still has the same eigenvalues as <m>A</m>.
        By some sort of dark magic, this sequence of matrices converges to an upper triangular matrix with eigenvalues on the diagonal!
      </p>

      <p>
        Consider the matrix <m>A = \begin{bmatrix}5&amp;-2&amp;3\\0&amp;4&amp;0\\0&amp;-1&amp;3\end{bmatrix}</m>
      </p>

      <sage>
        <input>
          A = Matrix(3,3,[5,-2,3,0,4,0,0,-1,3])
          A.eigenvals()
        </input>
        <output>
          \[\{3:1, 4:1, 5:1\}\]
        </output>
      </sage>

      <sage>
        <input>
          Q1,R1 = A.QRdecomposition()
          A2=R1*Q1
          A2,Q1,R1
        </input>
        <output>
          \[\left(\bbm 5\amp -\frac{11\sqrt{17}}{17}\amp \frac{10\sqrt{17}}{17}\\ 0\amp \frac{71}{17} \amp \frac{5}{17}\\ 0\amp -\frac{12}{17} \amp \frac{48}{17}\ebm, \bbm 1\amp 0\amp 0\\ 0\amp \frac{4\sqrt{17}}{17} \amp \frac{\sqrt{17}}{17}\\ 0\amp -\frac{\sqrt{17}}{17}, \frac{4\sqrt{17}}{17}\ebm, \bbm 5\amp -2\amp 3\\ 0\amp \sqrt{17}\amp -\frac{3\sqrt{17}}{17}\\ 0\amp 0\amp \frac{12\sqrt{17}}{17}\ebm\right)\]
        </output>
      </sage>

      <p>
        Now we repeat the process:
      </p>

      <sage>
        <input>
          Q2,R2 = A2.QRdecomposition()
          A3=R2*Q2
          A3.evalf()
        </input>
        <output>
          \[\bbm 5.0\amp -3.0347711718635 \amp 1.94683433666715\\ 0\amp 4.20655737704918 \amp 0.527868852459016\\ 0 \amp -0.472131147540984 \amp 2.79344262295082\ebm\]
        </output>
      </sage>

      <p>
        Do this a few more times, and see what results!
        (If someone can come up with a way to code this as a loop, let me know!)
        The diagonal entries should get closer to <m>5,4,3</m>, respectively,
        and the <m>(3,2)</m> entry should get closer to <m>0</m>.
      </p>

      <sage>

      </sage>

      <sage>

      </sage>
    </subsubsection>
  </subsection>

  <exercises>
    <exercise label="ex-ww-svd1">
      <webwork source="local/Library/Rochester/setLinearAlgebra24SingularValues/ur_la_24_1.pg"/>
    </exercise>
    <exercise label="ex-ww-svd2">
      <webwork source="local/Library/Rochester/setLinearAlgebra24SingularValues/ur_la_24_5.pg"/>
    </exercise>
    <exercise label="ex-ww-qr1">
      <webwork source="local/Library/Rochester/setLinearAlgebra19QRfactorization/ur_la_19_2.pg"/>
    </exercise>
    <exercise label="ex-ww-qr2">
      <webwork source="local/Library/Rochester/setLinearAlgebra19QRfactorization/ur_la_19_3.pg"/>
    </exercise>
  </exercises>
</section>
