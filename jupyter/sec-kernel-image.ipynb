{
"cells": [
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["%%html\n<link href=\"https:\/\/pretextbook.org\/beta\/mathbook-content.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/aimath.org\/mathbook\/mathbook-add-on.css\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Open+Sans:400,400italic,600,600italic\" rel=\"stylesheet\" type=\"text\/css\" \/>\n<link href=\"https:\/\/fonts.googleapis.com\/css?family=Inconsolata:400,700&subset=latin,latin-ext\" rel=\"stylesheet\" type=\"text\/css\" \/><!-- Hide this cell. -->\n<script>\nvar cell = $(\".container .cell\").eq(0), ia = cell.find(\".input_area\")\nif (cell.find(\".toggle-button\").length == 0) {\nia.after(\n    $('<button class=\"toggle-button\">Toggle hidden code<\/button>').click(\n        function (){ ia.toggle() }\n        )\n    )\nia.hide()\n}\n<\/script>\n"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["**Important:** to view this notebook properly you will need to execute the cell above, which assumes you have an Internet connection.  It should already be selected, or place your cursor anywhere above to select.  Then press the \"Run\" button in the menu bar above (the right-pointing arrowhead), or press Shift-Enter on your keyboard."]},
{"cell_type":"markdown", "metadata":{}, "source":["$\\newcommand{\\spn}{\\operatorname{span}}\n\\newcommand{\\bbm}{\\begin{bmatrix}}\n\\newcommand{\\ebm}{\\end{bmatrix}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Img}{\\operatorname{im}}\n\\newcommand{\\nll}{\\operatorname{null}}\n\\newcommand{\\csp}{\\operatorname{col}}\n\\newcommand{\\rank}{\\operatorname{rank}}\n\\newcommand{\\lt}{<}\n\\newcommand{\\gt}{>}\n\\newcommand{\\amp}{&}\n$"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><h6 class=\"heading hide-type\"><span class=\"type\">Section<\/span> <span class=\"codenumber\">3.2<\/span> <span class=\"title\">Kernel and Image<\/span><\/h6><a href=\"sec-kernel-image.ipynb\" class=\"permalink\">¶<\/a><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-187\">Given any linear transformation $T:V\\to W$ we can associate two important subspaces: the <dfn class=\"terminology\">kernel<\/dfn> of $T$ (also known as the <dfn class=\"terminology\">nullspace<\/dfn>), and the <dfn class=\"terminology\">image<\/dfn> of $T$ (also known as the <dfn class=\"terminology\">range<\/dfn>).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition-like\" id=\"def-kernel-image\"><h6 class=\"heading\"><span class=\"type\">Definition<\/span> <span class=\"codenumber\">3.2.1<\/span>.<\/h6><p id=\"p-188\">Let $T:V\\to W$ be a linear transformation. The <dfn class=\"terminology\">kernel<\/dfn> of $T\\text{,}$ denoted $\\ker T\\text{,}$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\ker T = \\{\\vec{v}\\in V \\,|\\, T(\\vec{v}=\\vec{0})\\}\\text{.}\n\\end{equation*}\n<\/div><p>The <dfn class=\"terminology\">image<\/dfn> of $T\\text{,}$ denoted $\\Img T\\text{,}$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\Img T = \\{(T\\vec{v}) \\,|\\, \\vec{v}\\in V\\}\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-189\">Note that the kernel of $T$ is just the set of all vectors $T$ sends to zero. The image of $T$ is the range of $T$ in the usual sense of the range of a function.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem-like\" id=\"thm-ker-img-subspace\"><h6 class=\"heading\"><span class=\"type\">Theorem<\/span> <span class=\"codenumber\">3.2.2<\/span>.<\/h6><p id=\"p-190\">For any linear transformation $T:V\\to W\\text{,}$<\/p><ol class=\"decimal\"><li id=\"li-36\"><p id=\"p-191\">$\\ker T$ is a subspace of $V\\text{.}$<\/p><\/li><li id=\"li-37\"><p id=\"p-192\">$\\Img T$ is a subspace of $W\\text{.}$<\/p><\/li><\/ol><\/article><article class=\"proof\" id=\"proof-8\"><h6 class=\"heading\"><span class=\"type\">Proof.<\/span><\/h6><ol id=\"p-193\" class=\"decimal\"><li id=\"li-38\"><p id=\"p-194\">To show that $\\ker T$ is a subspace, first note that $\\vec{0}\\in \\ker T\\text{,}$ since $T(\\vec{0})=\\vec{0}$ for any linear transformation $T\\text{.}$ If $\\vec{v},\\vec{w}\\in \\ker T\\text{,}$ then $T(\\vec{v})=\\vec{0}$ and $T(\\vec{w})=0\\text{,}$ and therefore,<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(\\vec{v}+\\vec{w})=T(\\vec{v})+T(\\vec{w})=\\vec{0}+\\vec{0}=\\vec{0}\\text{.}\n\\end{equation*}\n<\/div><p>Similarly, for any scalar $c$ and $\\vec{v}\\in \\ker T\\text{,}$<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(c\\vec{v})=cT(\\vec{v})=c\\vec{0}=\\vec{0}\\text{.}\n\\end{equation*}\n<\/div><p>By the subspace test, $\\ker T$ is a subspace.<\/p><\/li><li id=\"li-39\"><p id=\"p-195\">Again, since $T(\\vec{0})=\\vec{0}\\text{,}$ we see that $\\vec{0}\\in \\Img T\\text{,}$ so $\\Img T$ is nonempty. If $\\vec{w}_1,\\vec{w}_2\\in \\Img T\\text{,}$ then there exist $\\vec{v}_1,\\vec{v}_2\\in V$ such that $T(\\vec{v}_1)=\\vec{w}_1$ and $T(\\vec{v}_2)=\\vec{w}_2\\text{.}$ It follows that<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vec{w}_1+\\vec{w}_2 = T(\\vec{v}_1)+T(\\vec{v}_2) = T(\\vec{v}_1+\\vec{v}_2)\\text{,}\n\\end{equation*}\n<\/div><p>so $\\vec{w}_1+\\vec{w}_2\\in \\Img T\\text{.}$ Similarly, if $c$ is any scalar and $\\vec{w}=T(\\vec{v})\\in\\Img T\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nc\\vec{w}=cT(\\vec{v})=T(c\\vec{v})\\text{,}\n\\end{equation*}\n<\/div><p>so $c\\vec{w}\\in \\Img T\\text{.}$<\/p><\/li><\/ol><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-196\">A familiar setting that you may already have encontered in a previous linear algebra course is that of a matrix transformation. Let $A$ be an $m\\times n$ matrix. Then we can define $T:\\R^n\\to \\R^m$ by $T(\\vec{x})=A\\vec{x}\\text{,}$ where elements of $\\R^n,\\R^m$ are considered as column vectors. We then have<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\ker T = \\nll(A) = \\{\\vec{x}\\in \\R^n \\,|\\, A\\vec{x}=\\vec{0}\\}\n\\end{equation*}\n<\/div><p>and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\Img T = \\csp(A) = \\{A\\vec{x}\\,|\\, \\vec{x|\\in \\R^n}\\text{,}\n\\end{equation*}\n<\/div><p>where $\\csp(A)$ denotes the <dfn class=\"terminology\">column space<\/dfn> of $A\\text{.}$ Recall further that if we write $A$ in terms of its columns as<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA = \\bbm C_1 \\amp C_2 \\amp \\cdots \\amp C_n\\ebm\n\\end{equation*}\n<\/div><p>and a vector $\\vec{x}\\in \\R^n$ as $\\vec{x}=\\bbm x_1\\\\x_2\\\\\\vodts \\\\x_n\\ebm\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA\\vec{x} = x_1C_1+x_2C_2+\\cdots +x_nC_n\\text{.}\n\\end{equation*}\n<\/div><p>Thus, any element of $\\csp(A)$ is a linear combination of its columns, explaining the name <em class=\"emphasis\">column space<\/em>.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-197\">Determining $\\nll(A)$ and $\\csp(A)$ for a given matrix $A$ is, unsurprisingly, a matter of reducing $A$ to row-echelon form. Finding $\\nll(A)$ is simply a matter of describing the set of all solutions to the homogeneous system $A\\vec{x}=\\vec{0}\\text{.}$ Finding $\\csp(A)$ relies on the following theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem-like\" id=\"thm-colspace\"><h6 class=\"heading\"><span class=\"type\">Theorem<\/span> <span class=\"codenumber\">3.2.3<\/span>.<\/h6><p id=\"p-198\">Let $A$ be an $m\\times n$ matrix with columns $C_1,C_2,\\ldots, C_n\\text{.}$ If the reduced row-echelon form of $A$ has leading ones in columns $j_1,j_2,\\ldots, j_k\\text{,}$ then $\\{C_{j_1},C_{j_2},\\ldots, c_{j_k}\\}$ is a basis for $\\csp(A)\\text{.}$<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-199\">The truth of this theorem is demonstrated in Section 5.4 of the text by Nicholson. To see why it works, we need to remember a few basic facts from elementary linear algebra. First, recall that performing an elementary row operation on a matrix $A$ is equivalent to multiplying on the left by an elementary matrix $E$ defined using the same row operation.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-200\">Since every elementary matrix is invertible, and any product of invertible matrices is invertible, and we can transform $A$ into a row-echelon matrix $R$ using elementary row operations, it follows that $R = UA$ for an invertible matrix $U\\text{;}$ indeed, we have $U = E_kE_{k-1}\\cdots E_2E_1\\text{,}$ where $E_1,\\ldots, E_k$ are the elemetnary matrices corresponding to the row operations used to carry $A$ to $R\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-201\">A basis for $\\csp(R)$ is given by the columns of $R$ containing the leading ones. The reason for this is as follows. First, recall that each nonzero row begins with a leading one. So if the leading ones of $R$ are in columns $i_1,\\ldots, i_k\\text{,}$ then there are $k$ nonzero rows. Since all rows of zeros go at the bottom, each column in $R$ has its last $m-k$ entries identically zero. Thus,<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\csp(R)\\subseteq \\left\\{\\bbm a_1\\\\\\vdots \\\\a_k\\\\0\\\\\\vdots 0\\ebm\\in \\R^m \\,|\\, a_1,\\ldots, a_k\\in\\R\\right\\}\\text{,}\n\\end{equation*}\n<\/div><p>so $\\dim \\csp(R)\\leq k\\text{.}$ But the columns containing leading ones are easily shown to be independent, so they form a basis of $\\csp(R)\\text{,}$ which therefore has dimension $k=\\operatorname{rank}(A)\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-202\">Next, since $R=UA\\text{,}$ where $U$ is invertible, if $R=\\bbm Y_1\\amp Y_2\\amp \\cdots \\amp Y_n\\ebm$ and $A = \\bbm C_1\\amp C_2\\amp \\cdots \\amp C_n\\ebm\\text{,}$ then $C_i = U^{-1}Y_i$ for each $i\\text{.}$ It follows from the fact that $U$ is invertible and that the columns containing leading ones in $R$ form a basis for $\\csp(R)$ that the corresponding columns in $A$ form a basis for $\\csp(A)\\text{.}$ (For details, see Section 5.4 in Nicholson.)<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-203\">For example, consider the linear transformation $T:\\R^4\\to \\R^3$ defined by the matrix<\/p><div class=\"displaymath\">\n\\begin{equation*}\nA = \\bbm 1 \\amp 3 \\amp 0 \\amp -2\\\\\n-2 \\amp -1 \\amp 2 \\amp 0\\\\\n1 \\amp 8 \\amp 2 \\amp -6\\ebm\\text{.}\n\\end{equation*}\n<\/div><p>Let's determine the RREF of $A\\text{:}$<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["from sympy import *\ninit_printing()\nA=Matrix(3,4,[1,3,0,-2,-2,-1,2,0,1,8,2,-6])\nA.rref()"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-204\">We see that there are leading ones in the first and second column. Therefore, $\\csp(A) = \\Img(T) = \\spn\\left\\{\\bbm 1\\\\-2\\\\1\\ebm, \\bbm 3\\\\-1\\\\8\\ebm\\right\\}\\text{.}$ Indeed, note that<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\bbm 0\\\\2\\\\2\\ebm = -\\frac65\\bbm 1\\\\-2\\\\1\\ebm + \\frac25\\bbm 3\\\\-1\\\\8\\ebm\n\\end{equation*}\n<\/div><p>and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\bbm -2\\\\0\\\\-6\\ebm = \\frac25\\bbm 1\\\\-2\\1\\ebm -\\frac45\\bbm 3\\\\-1\\\\8\\ebm\\text{,}\n\\end{equation*}\n<\/div><p>so that indeed, the third and fourth columns are in the span of the first and second.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-205\">Furthermore, we can determine the nullspace: if $A\\vec{x}=\\vec{0}$ where $\\vec{x}=\\bbm x_1\\\\x_2\\\\x_3\\\\x_4\\ebm\\text{,}$ then we must have<\/p><div class=\"displaymath\">\n\\begin{align*}\nx_1 \\amp =\\frac65 x_3-\\frac25 x_4\\\\\nx_2 \\amp =-\\frac25 x_3+\\frac 45 x_4\\text{,}\n\\end{align*}\n<\/div><p>so<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vec{x} = \\bbm \\frac65x_3-\\frac25x_4\\\\ -\\frac25x_3+\\frac45x_4\\\\x_3\\\\x_4\\ebm = \\frac{x_3}{5}\\bbm 6\\\\-2\\\\5\\\\0\\ebm + \\frac{x_4}{5}\\bbm -2\\\\4\\\\0\\\\5\\ebm\\text{.}\n\\end{equation*}\n<\/div><p>It follows that a basis for $\\nll(A)=\\ker T$ is $\\left\\{\\bbm 6\\\\-2\\\\5\\\\0\\ebm, \\bbm -2\\\\4\\\\0\\\\5\\ebm\\right\\}\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-206\">Incidentally, the SymPy library for Python has built-in functions for computing nullspace and column space. But it's probably worth your while to know how to determine these from the RREF of a matrix, without additional help from the computer. That said, let's see how the computer's output compares to what we found:<\/p><\/div>"]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.nullspace()"], "outputs":[]},
{"cell_type":    "code", "execution_count":null, "metadata":{}, "source":["A.columnspace()"], "outputs":[]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-207\">Note that the output from the computer simply states the basis for each space. Of course, for computational purposes, this is typically good enough.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-208\">An important result that comes out while trying to show that the “pivot columns” of a matrix (the ones that end up with leading ones in the RREF) are a basis for the column space is that the column rank (defined as the dimesion of $\\csp(A)$) and the row rank (the dimension of the space spanned by the columns of $A$) are equal. One can therefore speak unabmiguously about the <dfn class=\"terminology\">rank<\/dfn> of a matrix $A\\text{,}$ and it is just as it's defined in a first course in linear algebra: the number of leading ones in the RREF of $A\\text{.}$<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-209\">For a general linear transformation, we can't necessarily speak in terms of rows and columns, but if $T:V\\to W$ is linear, and either $V$ or $W$ is finite-dimensional, then we can define the rank of $T$ as follows.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"definition-like\" id=\"def-rank-transformation\"><h6 class=\"heading\"><span class=\"type\">Definition<\/span> <span class=\"codenumber\">3.2.4<\/span>.<\/h6><p id=\"p-210\">Let $T:V\\to W$ be a linear transformation. Then the <dfn class=\"terminology\">rank<\/dfn> of $T$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\operatorname{rank} T = \\dim \\Img T\\text{,}\n\\end{equation*}\n<\/div><p>and the <dfn class=\"terminology\">nullity<\/dfn> of $T$ is defined by<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\operatorname{nullity} T = \\dim \\ker T\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-211\">Note that if $W$ is finite-dimensional, then so is $\\Img T\\text{,}$ since it's a subspace of $W\\text{.}$ On the other hand, if $V$ is finite-dimensional, then we can find a basis $\\{\\vec{v}_1,\\ldots, \\vec{v}_n}}$ of $V\\text{,}$ and the set $\\{T(\\vec{v}_1),\\ldots, T(\\vec{v}_n\\})\\}$ will span $\\Img T\\text{,}$ so again the image is finite-dimensional, so the rank of $T$ is finite. It is possible for either the rank or the nullity of a transformation to be infinite.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-212\">Knowing that the kernel and image of an operator are subspaces gives us an easy way to define subspaces. From the textbook, we have the following nice example.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"example-like\" id=\"example-2\"><h6 class=\"heading\"><span class=\"type\">Example<\/span> <span class=\"codenumber\">3.2.5<\/span>.<\/h6><p id=\"p-213\">Let $T:M_{nn}\\to M_{nn}$ be defined by $T(A)=A-A^T\\text{.}$ Then<\/p><ol class=\"decimal\"><li id=\"li-40\"><p id=\"p-214\">$T$ is a linear map.<\/p><\/li><li id=\"li-41\"><p id=\"p-215\">$\\ker T$ is equal to the set of all symmetric matrices.<\/p><\/li><li id=\"li-42\"><p id=\"p-216\">$\\Img T$ is equal to the set of all skew-symmetric matrices.<\/p><\/li><\/ol><div class=\"solutions\"><a data-knowl=\"\" class=\"id-ref\" data-refid=\"hk-solution-8\" id=\"solution-8\"><span class=\"type\">Solution<\/span><\/a><div id=\"hk-solution-8\" class=\"hidden-content tex2jax_ignore\"><div class=\"solution\"><ol id=\"p-217\" class=\"decimal\"><li id=\"li-43\"><p id=\"p-218\">We have $T(0)=0$ since $0^T=0\\text{.}$ Using proerties of the transpose and matrix algebra, we have<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(A+B) = (A+B)-(A+B)^T = (A-A^T)+(B-B^T) = T(A)+T(B)\n\\end{equation*}\n<\/div><p>and<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT(kA) = (kA) - (kA)^T = kA-kA^T = k(A-A^T) = kT(A)\\text{.}\n\\end{equation*}\n<\/div><\/li><li id=\"li-44\"><p id=\"p-219\">It's clear that if $A^T=A\\text{,}$ then $T(A)=0\\text{.}$ On the other hand, if $T(A)=0\\text{,}$ then $A-A^T=0\\text{,}$ so $A=A^T\\text{.}$ Thus, the kernel consists of all symmetric matrices.<\/p><\/li><li id=\"li-45\"><p id=\"p-220\">If $B=T(A)=A-A^T\\text{,}$ then<\/p><div class=\"displaymath\">\n\\begin{equation*}\nB^T = (A-A^T)^T = A^T-A = -B\\text{,}\n\\end{equation*}\n<\/div><p>so certainly every matrix in $\\Img A$ is skew-symmetric. On the other hand, if $B$ is skew-symmetric, then $B=T(\\frac12 B)\\text{,}$ since<\/p><div class=\"displaymath\">\n\\begin{equation*}\nT\\Bigl(\\frac12 B\\Bigr) = \\frac12 T(B) = \\frac12(B-B^T) = \\frac12(B-(-B))= B\\text{.}\n\\end{equation*}\n<\/div><\/li><\/ol><\/div><\/div><\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-221\">You'll recall from a course like Math 2000 that in the study of functions, the properties of being injective (one-to-one) and surjective (onto) are important. They're important for linear transformations as well, and defined in exactly the same way.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-222\">It's clear that being surjective is closely tied to image. Indeed, by definition, $T:V\\to W$ is onto if $\\Img T = W\\text{.}$ What might not be immediately obvious is that the kernel tells us if a linear map is injective.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem-like\" id=\"thm-injective-kernel\"><h6 class=\"heading\"><span class=\"type\">Theorem<\/span> <span class=\"codenumber\">3.2.6<\/span>.<\/h6><p id=\"p-223\">Let $T:V\\to W$ be a linear transformation. Then $T$ is injective if and only if $\\ker T = \\{\\vec{0}\\}\\text{.}$<\/p><\/article><article class=\"proof\" id=\"proof-9\"><h6 class=\"heading\"><span class=\"type\">Proof.<\/span><\/h6><p id=\"p-224\">Suppose $T$ is injective, and let $\\vec{v}\\in \\ker T\\text{.}$ Then $T(\\vec{v})=\\vec{0}\\text{.}$ On the other hand, we know that $T(\\vec{0})=\\vec{0}\\text{,}$ and since $T$ is injective, we must have $\\vec{v}=\\vec{0}\\text{.}$ Conversely, suppose that $\\ker T = \\{0\\}$ and that $T(\\vec{v}_1)=T(\\vec{v}_2)$ for some $\\vec{v}_1,\\vec{v}_2\\in V\\text{.}$ Then<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\vec{0} = T(\\vec{v}_1)-T(\\vec{v}_2) = T(\\vec{v}_1-\\vec{v}_2)\\text{,}\n\\end{equation*}\n<\/div><p>so $\\vec{v}_1-\\vec{v}_2\\in \\ker T\\text{.}$ Therefore, we must have $\\vec{v}_1-\\vec{v}_2=\\vec{0}\\text{,}$ so $\\vec{v}_1=\\vec{v}_2\\text{,}$ and it follows that $T$ is injective.<\/p><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-225\">Let us return to the case of a matrix transformation $T_A:\\R^n\\to \\R^m\\text{.}$ Notice that $\\ker T_A$ is simply the set of all solutions to $A\\vec{x}=\\vec{0}\\text{,}$ while $\\Img T_A$ is the set of all $\\vec{y}\\in\\R^m$ for which $A\\vec{x}=\\vec{y}$ <em class=\"emphasis\">has<\/em> a solution.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-226\">Recall from the discussion above that $\\rank A = \\dim \\csp(A) = \\dim \\Img T_A\\text{.}$ It follows that $T_A$ is surjective if and only if $\\rank A = m\\text{.}$ On the other hand, $T_A$ is injective if and only if $\\rank A = n\\text{,}$ because we know that the system $A\\vec{x}=\\vec{0}$ has a unique solution if and only if each column of $A$ contains a leading one.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-227\">This has some interesting consequences. If $m=n$ (that is, if $A$ is square), then each increase in $\\dim \\nll(A)$ produces a corresponding decrease in $\\dim \\csp(A)\\text{,}$ since both correspond to the “loss” of a leading one. Moreover, if $\\rank A = n\\text{,}$ then $T_A$ is both injective and surjective. Recall that a function is invertible if and only if it is both injective and surjective. It should come as no surprise that invertibility of $T_A$ (as a function) is equivalent to invertibility of $A$ (as a matrix).<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-228\">Also, note that if $m \\lt n\\text{,}$ then $\\rank A\\leq m \\lt n\\text{,}$ so $T_A$ could be surjective, but can't possibly be injective. On the other hand, if $m\\gt n\\text{,}$ then $\\rank A\\leq n \\lt m\\text{,}$ so $T_A$ could be injective, but can't possibly be surjective. These results generalize to linear transformations between any finite-dimensional vector spaces. The first step towards this is the following fundamental theorem.<\/p><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><article class=\"theorem-like\" id=\"thm-dimension-lintrans\"><h6 class=\"heading\"><span class=\"type\">Theorem<\/span> <span class=\"codenumber\">3.2.7<\/span>. <span class=\"title\">Dimension Theorem (Fundamental Theorem of Linear Transformations).<\/span><\/h6><p id=\"p-229\">Let $T:V\\to W$ be any linear transformation such that $\\ker T$ and $\\Img T$ are finite-dimensional. Then $V$ is finite-dimensional, and<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\dim V = \\dim \\ker T + \\dim \\Img T\\text{.}\n\\end{equation*}\n<\/div><\/article><\/div>"]},
{"cell_type":"markdown", "metadata":{}, "source":["<div class=\"mathbook-content\"><p id=\"p-230\">This is sometimes known as the <em class=\"emphasis\">Rank-Nullity Theorem<\/em>, since it can be stated in the form<\/p><div class=\"displaymath\">\n\\begin{equation*}\n\\dim V = \\rank T + \\operatorname{nullity} T\\text{.}\n\\end{equation*}\n<\/div><\/div>"]}
],
"nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "", "name": "sagemath"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}, "name": "sec-kernel-image.ipynb"}
}